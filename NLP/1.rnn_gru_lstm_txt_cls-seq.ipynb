{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a35057",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE    = 20000\n",
    "MAX_SEQUENCE_LEN  = 200\n",
    "EMBEDDING_DIM     = 128\n",
    "RNN_UNITS         = 64\n",
    "BATCH_SIZE        = 64\n",
    "EPOCHS            = 5                 # bumped up for meaningful training\n",
    "AUTOTUNE          = tf.data.AUTOTUNE\n",
    "NUM_CLASSES       = 4\n",
    "CLASS_NAMES       = [\"World\",\"Sports\",\"Business\",\"Sci/Tech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea44913",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8363033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    \"C:/Users/skhan4/Documents/data/ag_news_train.csv\",\n",
    "    header=0               # first row is the actual header\n",
    ")\n",
    "train_df = train_df.rename(\n",
    "    columns={\"Class Index\":\"label\",\"Title\":\"title\",\"Description\":\"description\"}\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    \"C:/Users/skhan4/Documents/data/ag_news_test.csv\",\n",
    "    header=0\n",
    ")\n",
    "test_df = test_df.rename(\n",
    "    columns={\"Class Index\":\"label\",\"Title\":\"title\",\"Description\":\"description\"}\n",
    ")\n",
    "\n",
    "\n",
    "# zero-based labels\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int) - 1\n",
    "test_df[\"label\"]  = test_df[\"label\"].astype(int) - 1\n",
    "\n",
    "# combine title + description\n",
    "train_df[\"text\"] = train_df[\"title\"] + \" \" + train_df[\"description\"]\n",
    "test_df[\"text\"]  = test_df[\"title\"]  + \" \" + test_df[\"description\"]\n",
    "\n",
    "# split out a validation set\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"text\"].values,\n",
    "    train_df[\"label\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"label\"].values\n",
    ")\n",
    "test_texts = test_df[\"text\"].values\n",
    "test_labels = test_df[\"label\"].values\n",
    "\n",
    "\n",
    "print(\"train_texts.shape:\", train_texts.shape)\n",
    "print(\"val_texts.shape:  \", val_texts.shape)\n",
    "print(\"train_labels.shape:\", train_labels.shape)\n",
    "print(\"val_labels.shape:  \", val_labels.shape)\n",
    "print(\"test_texts.shape: \", test_texts.shape)\n",
    "print(\"test_labels.shape:\", test_labels.shape)\n",
    "\n",
    "\"\"\"\n",
    "train_texts.shape: (3,)\n",
    "val_texts.shape:   (2,)\n",
    "train_labels.shape: (3,)\n",
    "val_labels.shape:   (2,)\n",
    "\n",
    "train_texts: [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"TensorFlow makes data pipelines easy.\",\n",
    "    \"I enjoy building neural networks.\"\n",
    "]\n",
    "val_texts: [\n",
    "    \"Keras simplifies model saving and loading.\",\n",
    "    \"AI models require large datasets.\"\n",
    "]\n",
    "train_labels: [0, 1, 0]\n",
    "val_labels:   [0, 1]\n",
    "\"\"\"\n",
    "\n",
    "# val_labels.shape:   (24000,)\n",
    "# test_texts.shape:  (7600,)\n",
    "# test_labels.shape: (7600,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040294ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) Toy sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog ate my homework.\",\n",
    "    \"Deep learning is fun.\"\n",
    "]\n",
    "\n",
    "# 2) Create & adapt the TextVectorization layer\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=None,                # keep up to top-10 tokens\n",
    "    output_mode=\"int\",            # map each token → an integer\n",
    "    output_sequence_length=8      # pad/truncate every output to length 5\n",
    ")\n",
    "vectorizer.adapt(sentences)\n",
    "\n",
    "# 3) Inspect the learned vocabulary\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "print(\"vocabulary :\", vocab)\n",
    "# e.g. ['','[UNK]','the','cat','sat','on','mat','dog']\n",
    "\n",
    "# 4) Vectorize a batch of sentences\n",
    "batch = tf.constant(sentences)[:, tf.newaxis]   # shape (3,1)\n",
    "token_ids = vectorizer(batch)\n",
    "print(\"token IDs:\\n\", token_ids.numpy())\n",
    "\n",
    "\n",
    "\n",
    "vocabulary : ['', '[UNK]', 'the', 'sat', 'on', 'my', 'mat', 'learning', 'is', 'homework', 'fun', 'dog', 'deep', 'cat', 'ate']\n",
    "token IDs:\n",
    " [[ 2 13  3  4  2  6  0  0]\n",
    " [ 2 11 14  5  9  0  0  0]\n",
    " [12  7  8 10  0  0  0  0]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9cbb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_texts) #counts how often each token appears, and then picks the top MAX_VOCAB_SIZE–2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ca37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    tokens = vectorizer(tf.expand_dims(text, -1))\n",
    "    return tf.squeeze(tokens, axis=0), label\n",
    "\n",
    "\n",
    "\n",
    "def make_dataset(texts, labels, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    print(type(ds))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(texts), seed=42)\n",
    "    return ds.map(vectorize_text, num_parallel_calls=AUTOTUNE) \\\n",
    "             .batch(BATCH_SIZE) \\\n",
    "             .prefetch(AUTOTUNE)\n",
    "\n",
    "\"\"\"\n",
    "After map:\n",
    "[ ( [2,3,4,5,2,7], 0 ),\n",
    "  ( [2,6,4,5,2,8], 1 ),\n",
    "  ( [9,10,11,0,0,0], 0 ) ]\n",
    "\n",
    "After batch (size=2):\n",
    "(\n",
    "  [[2,3,4,5,2,7],\n",
    "   [2,6,4,5,2,8]],    # shape (2,6)\n",
    "  [0,1]               # shape (2,)\n",
    "), \n",
    "(\n",
    "  [[9,10,11,0,0,0]],  # the last batch (could drop or pad depending on drop_remainder)\n",
    "  [0]\n",
    ")\n",
    "\n",
    "With prefetch:\n",
    "While your model is training on batch #1, batch #2 is already being prepared in the background\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_texts, train_labels, shuffle=True)\n",
    "val_ds   = make_dataset(val_texts,   val_labels)\n",
    "test_ds  = make_dataset(test_texts,  test_labels)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sentences = [\n",
    "    \"the cat sat\",\n",
    "    \"dog runs fast\",\n",
    "    \"hello world\",\n",
    "    \"tensorflow rocks\",\n",
    "    \"deep learning\",\n",
    "    \"vectorization layer\"\n",
    "]\n",
    "labels = [0, 1, 0, 1, 0, 1]\n",
    "\n",
    "MAX_VOCAB_SIZE   = 20\n",
    "MAX_SEQUENCE_LEN = 4\n",
    "BATCH_SIZE       = 2\n",
    "\n",
    "Batch 1:\n",
    "  x_batch shape: (2, 4)\n",
    "  y_batch shape: (2,)\n",
    "  x_batch values:\n",
    "   [[ 2  3  4  0]    # e.g. [\"the\",\"cat\",\"sat\", <pad>]\n",
    "    [ 5  6  7  0]]   # e.g. [\"dog\",\"runs\",\"fast\", <pad>]\n",
    "  y_batch values: [0 1]\n",
    "\n",
    "Batch 2:\n",
    "  x_batch shape: (2, 4)\n",
    "  y_batch shape: (2,)\n",
    "  x_batch values:\n",
    "   [[ 8  9  0  0]   # e.g. [\"hello\",\"world\",<pad>,<pad>]\n",
    "    [10 11  0  0]]  # e.g. [\"tensorflow\",\"rocks\",<pad>,<pad>]\n",
    "  y_batch values: [0 1]\n",
    "\n",
    "Batch 3:\n",
    "  x_batch shape: (2, 4)\n",
    "  y_batch shape: (2,)\n",
    "  x_batch values:\n",
    "   [[12 13  0  0]   # e.g. [\"deep\",\"learning\",<pad>,<pad>]\n",
    "    [14 15  0  0]]  # e.g. [\"vectorization\",\"layer\",<pad>,<pad>]\n",
    "  y_batch values: [0 1]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "sentences = [\n",
    "    \"the cat sat\",\n",
    "    \"dog runs fast\",\n",
    "    \"hello world\",\n",
    "    \"tensorflow rocks\",\n",
    "    \"deep learning\",\n",
    "    \"vectorization layer\"\n",
    "]\n",
    "have exactly 14 distinct words\n",
    "\n",
    "[\"the\", \"cat\", \"sat\", \"dog\", \"runs\", \"fast\",\n",
    " \"hello\", \"world\", \"tensorflow\", \"rocks\",\n",
    " \"deep\", \"learning\", \"vectorization\", \"layer\"]\n",
    "\n",
    "\n",
    "\tdim_0\tdim_1\tdim_2\n",
    "\t0.496714153\t-0.138264301\t0.647688538\n",
    "[UNK]\t1.523029856\t-0.234153375\t-0.234136957\n",
    "the\t1.579212816\t0.767434729\t-0.469474386\n",
    "cat\t0.542560044\t-0.463417693\t-0.465729754\n",
    "sat\t0.241962272\t-1.913280245\t-1.724917833\n",
    "dog\t-0.562287529\t-1.01283112\t0.314247333\n",
    "runs\t-0.908024076\t-1.412303701\t1.465648769\n",
    "fast\t-0.2257763\t0.067528205\t-1.424748186\n",
    "hello\t-0.544382725\t0.11092259\t-1.150993577\n",
    "world\t0.375698018\t-0.60063869\t-0.29169375\n",
    "tensorflow\t-0.601706612\t1.852278185\t-0.013497225\n",
    "rocks\t-1.057710929\t0.822544912\t-1.22084365\n",
    "deep\t0.208863595\t-1.959670124\t-1.328186049\n",
    "learning\t0.196861236\t0.73846658\t0.171368281\n",
    "vectorization\t-0.115648282\t-0.301103696\t-1.47852199\n",
    "layer\t-0.719844208\t-0.460638771\t1.057122226\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"C:/Users/skhan4/Documents/data\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\"\"\" \n",
    "# Suppose our “embedded” input sequence is:\n",
    "# shape = (1, 3, 2)\n",
    "x = [[[0.5, -0.1],\n",
    "      [1.0,  0.2],\n",
    "      [-0.3, 0.8]]]\n",
    "# A SimpleRNN with 4 units will:\n",
    "# 1) Initialize h0 = [0,0,0,0]\n",
    "# 2) Step through t=1..3 computing hidden states h1, h2, h3,\n",
    "#    each a length-4 vector. Internally:\n",
    "#      hi = activation( x_t · W_x + h_{i-1} · W_h + b )\n",
    "#\n",
    "# The final output is h3 of shape (1, 4). For example:\n",
    "h3 = [[ 0.12, -0.33, 0.47,  0.05 ]]  # ← shape (1, 4)\n",
    "\n",
    "Input to simpleRNN:\n",
    "[[[0.5, -0.1],\n",
    "  [1.0,  0.2],\n",
    "  [-0.3, 0.8]]]              # shape (1, 3, 2)\n",
    "\n",
    "Output of simpleRNN:  \n",
    "[[0.9306,−0.9163,0.9180,−0.8475]]  # shape (1, 4)\n",
    "\n",
    "net₁ = x₁·Wₓ + h₀·Wₕ + b\n",
    "     = [0.5, -0.1]·Wₓ + [0,0,0,0]·Wₕ + b\n",
    "     ≈ [ 2.965025  ,  -1.1565593,   0.4401187,   0.9483985]\n",
    "h₁ = tanh(net₁) ≈ [ 0.9946974, -0.8199154, 0.4137428, 0.73905715 ]\n",
    "\n",
    "net₂ = x₂·Wₓ + h₁·Wₕ + b\n",
    "\n",
    "t=1\n",
    "h1 = tanh(net1)=[0.9947,−0.8199,0.4137,0.7391]  \n",
    "t=2\n",
    "h2 = tanh(net2)=[0.9841,−0.4951,0.9428,0.9799]\n",
    "t=3\n",
    "h3 = tanh(net3)=[0.9306,−0.9163,0.9180,−0.8475]\n",
    "Final output\n",
    "h3=[0.9306,−0.9163,0.9180,−0.8475]\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f90a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([layers.Embedding(input_dim=MAX_VOCAB_SIZE,                                   output_dim=EMBEDDING_DIM,\n",
    "                                    input_length=MAX_SEQUENCE_LEN,mask_zero=True),\n",
    "        layers.SimpleRNN(RNN_UNITS),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "\"\"\"\n",
    "# output of SimpleRNN for two samples\n",
    "h = [[0.93, -0.92, 0.92, -0.85],\n",
    "     [0.10,  0.20, 0.30,  0.40]]   # shape (2,4)\n",
    "Dropout(0.5)\n",
    "h_drop = [[0.00, -0.92, 0.00, -0.85],\n",
    "          [0.10,  0.00, 0.30,  0.00]]   # still (2,4)\n",
    "Dense(3, softmax)\n",
    "# [[0.10, 0.85, 0.05],\n",
    "#  [0.33, 0.33, 0.34]]          \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b362e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "model.summary()\n",
    "\n",
    "ckpt = callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(ckpt_dir, f\"agnews_simpleRNN_1.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\"\n",
    "    )\n",
    "es = callbacks.EarlyStopping(\n",
    "    restore_best_weights=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=1,\n",
    "        callbacks=[ckpt, es]\n",
    "    )\n",
    "\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"SimpleRNN Test accuracy: {acc:.4f}\")\n",
    "\n",
    "model.save(os.path.join(ckpt_dir, \"agnews_simpleRNN_1.keras\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"C:/Users/skhan4/Documents/data\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "def build_and_train(model_name, recurrent_layer):\n",
    "    print(f\"\\n>>> Training {model_name}\")\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=MAX_VOCAB_SIZE,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_SEQUENCE_LEN,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        recurrent_layer,\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    ckpt = callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(ckpt_dir, f\"agnews_{model_name}.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\"\n",
    "    )\n",
    "    es = callbacks.EarlyStopping(\n",
    "        restore_best_weights=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=2\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[ckpt, es]\n",
    "    )\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(f\"{model_name} Test accuracy: {acc:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn_model  = build_and_train(\"simple_rnn\", layers.SimpleRNN(RNN_UNITS))\n",
    "gru_model  = build_and_train(\"gru\",        layers.GRU(RNN_UNITS))\n",
    "lstm_model = build_and_train(\"lstm\",       layers.LSTM(RNN_UNITS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d47706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(text, model):\n",
    "    seq   = vectorizer(tf.constant([text]))\n",
    "    probs = model.predict(seq)[0]\n",
    "    idx   = int(tf.argmax(probs))\n",
    "    return CLASS_NAMES[idx], float(probs[idx])\n",
    "\n",
    "examples = [\n",
    "    \"NASA launches new rover to explore Mars.\",\n",
    "    \"Champions League final ends in dramatic upset.\",\n",
    "    \"Federal Reserve hikes interest rates for third time.\",\n",
    "    \"Breakthrough in AI promises better natural language understanding.\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample predictions with LSTM model:\")\n",
    "for t in examples:\n",
    "    cls, conf = predict(t, lstm_model)\n",
    "    print(f\"{cls:<10} ({conf:.1%}): {t[:60]}…\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43161092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
