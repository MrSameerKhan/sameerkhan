{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a12179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Imports & Constants\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9c3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable hyperparameters\n",
    "MAX_VOCAB_SIZE    = 20_000    # keep top 20K tokens\n",
    "MAX_SEQUENCE_LEN  = 200       # pad or truncate all sequences to length 200\n",
    "EMBEDDING_DIM     = 128       # dimension of embedding vectors\n",
    "LSTM_UNITS        = 64        # number of units in LSTM layer\n",
    "BATCH_SIZE        = 64\n",
    "EPOCHS            = 10\n",
    "AUTOTUNE          = tf.data.AUTOTUNE\n",
    "NUM_CLASSES       = 4         # AG News has 4 categories\n",
    "\n",
    "CLASS_NAMES = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8e1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Load CSVs into Pandas\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# Files downloaded via get_file or otherwise placed in working dir\n",
    "train_df = pd.read_csv(\"D:/AIML/data/ag_news_train.csv\", header=None, names=[\"label\",\"title\",\"description\"])\n",
    "test_df  = pd.read_csv(\"D:/AIML/data/ag_news_test.csv\",  header=None, names=[\"label\",\"title\",\"description\"])\n",
    "\n",
    "# Zero-base the labels (1→0, 2→1, …, 4→3)\n",
    "train_df[\"label\"] -= 1\n",
    "test_df[\"label\"]  -= 1\n",
    "\n",
    "# Combine title + description into a single text column\n",
    "train_df[\"text\"] = train_df[\"title\"].str.cat(train_df[\"description\"], sep=\" \")\n",
    "test_df[\"text\"]  = test_df[\"title\"].str.cat(test_df[\"description\"], sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fd3d15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      2  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      2  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      2    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "\n",
       "                                         description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "\n",
       "                                                text  \n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...  \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c705cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7e6e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000,) (96000,)\n",
      "(24000,) (24000,)\n",
      "(7600,) (7600,)\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Split train → (train, validation)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df[\"text\"].values,\n",
    "    train_df[\"label\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"label\"].values\n",
    ")\n",
    "\n",
    "# Prepare test set\n",
    "test_texts  = test_df[\"text\"].values\n",
    "test_labels = test_df[\"label\"].values\n",
    "\n",
    "print(train_texts.shape, train_labels.shape)\n",
    "print(val_texts.shape, val_labels.shape)\n",
    "print(test_texts.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911a119d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ntext_input = tf.constant(\"hello unknown world\")\\nlabel_input = tf.constant(1)   # e.g. class 1\\n\\n[b\"hello unknown world\"]  shape=(1,)  \\n\\n#([2,1,3,0,0,0], 1)\\n\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Text Vectorization Layer\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_SEQUENCE_LEN\n",
    ")\n",
    "# Adapt to training texts only\n",
    "vectorizer.adapt(train_texts)\n",
    "\n",
    "\"\"\" \n",
    "# 3) Inspect the first 10 entries of the built vocabulary\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "print(\"Vocabulary (top 10):\", vocab[:10])\n",
    "\n",
    "# 4) Turn each sample text into its integer sequence\n",
    "vecs = vectorizer(sample_texts).numpy()\n",
    "for text, seq in zip(sample_texts, vecs):\n",
    "    print(f\"\\\"{text}\\\" → {seq}\")\n",
    "\n",
    "Vocabulary (top 10): ['', '[UNK]', 'the', 'was', 'movie', 'film', 'great', 'hated', 'plot', 'boring']\n",
    "\n",
    "\"the movie was great\"  → [2 4 3 6 0 0]\n",
    "\"I hated the film\"      → [1 7 2 5 0 0]\n",
    "\"the plot was boring\"   → [2 8 3 9 0 0]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    # 1) Add a “batch” dimension so vectorizer can process it:\n",
    "    #    text is a scalar tf.Tensor of dtype string, e.g. tf.Tensor(b\"hello world\", shape=(), dtype=string)\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    #    now it’s tf.Tensor([b\"hello world\"], shape=(1,), dtype=string)\n",
    "\n",
    "    # 2) Run through the TextVectorization layer:\n",
    "    #    produces shape (1, MAX_SEQUENCE_LEN) of int32 token IDs\n",
    "    token_ids = vectorizer(text)\n",
    "\n",
    "    # 3) Remove that extra batch dimension:\n",
    "    #    squeeze along axis 0 turns it into a 1-D tensor of length MAX_SEQUENCE_LEN\n",
    "    return tf.squeeze(token_ids, axis=0), label\n",
    "\n",
    "\"\"\" \n",
    "text_input = tf.constant(\"hello unknown world\")\n",
    "label_input = tf.constant(1)   # e.g. class 1\n",
    "\n",
    "[b\"hello unknown world\"]  shape=(1,)  \n",
    "\n",
    "pads to length 6 → [2,1,3,0,0,0]\n",
    "\n",
    "#([2,1,3,0,0,0], 1) ready for batching into a tf.data.Dataset.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f4b5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Build tf.data Pipelines\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "def make_dataset(texts, labels, shuffle=False):\n",
    "    # 1) Create a Dataset of individual (text, label) pairs\n",
    "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    # 2) Optionally shuffle the entries\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(texts), seed=42)\n",
    "    # 3) Convert each raw string → integer sequence, keep the label\n",
    "    ds = ds.map(vectorize_text, num_parallel_calls=AUTOTUNE)\n",
    "    # 4) Group into batches of BATCH_SIZE and prefetch for performance\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(train_texts, train_labels, shuffle=True)\n",
    "val_ds   = make_dataset(val_texts,   val_labels)\n",
    "test_ds  = make_dataset(test_texts,  test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fdca8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token batch shape: (64, 200)\n",
      "Label batch shape: (64,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "texts  = [\"hello world\", \"goodbye world\"]\n",
    "labels = [0, 1]   # two classes\n",
    "BATCH_SIZE = 2\n",
    "MAX_SEQUENCE_LEN = 4  # for simplicity\n",
    "\n",
    "and assume our vectorize_text will map\n",
    "\"hello world\" → [2,3,0,0]\n",
    "\n",
    "\"goodbye world\" → [4,3,0,0]\n",
    "\n",
    "1.from_tensor_slices\n",
    "(\"hello world\", 0)\n",
    "(\"goodbye world\", 1)\n",
    "\n",
    "2.map(vectorize_text)\n",
    "([2,3,0,0], 0)\n",
    "([4,3,0,0], 1)\n",
    "\n",
    "3. batch(2)\n",
    "Groups both elements into a single batch:\n",
    "sequences become a tensor of shape (2, 4)\n",
    "[[2,3,0,0],\n",
    " [4,3,0,0]]\n",
    "\n",
    "labels become a tensor of shape (2,)\n",
    "[0, 1]\n",
    "\n",
    "4. prefetch(AUTOTUNE)\n",
    "Overlaps data-preparation with model training for maximum throughput\n",
    "\n",
    "Token batch:\n",
    " [[2 3 0 0]\n",
    "  [4 3 0 0]]\n",
    "Token batch shape: (2, 4)\n",
    "\n",
    "Label batch:\n",
    " [0 1]\n",
    "Label batch shape: (2,)\n",
    "\n",
    "\"\"\"\n",
    "# Take one batch from the dataset\n",
    "for token_batch, label_batch in train_ds.take(1):\n",
    "    print(\"Token batch shape:\", token_batch.shape)\n",
    "    print(\"Label batch shape:\", label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dc0d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 128)          2560000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2667332 (10.18 MB)\n",
      "Trainable params: 2667332 (10.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Define Embedding + BiLSTM Model\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=MAX_VOCAB_SIZE,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_SEQUENCE_LEN,\n",
    "        mask_zero=True \n",
    "    ), #Input → shape (batch, MAX_SEQUENCE_LEN), Output → (batch, MAX_SEQUENCE_LEN, EMBEDDING_DIM)\n",
    "    layers.Bidirectional(layers.LSTM(LSTM_UNITS)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\"\"\" \n",
    "Input: integer sequences (batch, MAX_SEQUENCE_LEN)\n",
    "\n",
    "Embedding: → (batch, MAX_SEQUENCE_LEN, EMBEDDING_DIM)\n",
    "\n",
    "BiLSTM: → (batch, LSTM_UNITS*2)\n",
    "\n",
    "Dense+Dropout: → (batch, 64)\n",
    "\n",
    "Output: (batch, NUM_CLASSES)\n",
    "\"\"\"\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619014da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 7) Callbacks\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    \"D:/AIML/data/best_agnews_bilstm.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True\n",
    ")\n",
    "earlystop_cb  = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d46697ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.3888 - accuracy: 0.8719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 412s 269ms/step - loss: 0.3888 - accuracy: 0.8719 - val_loss: 0.2585 - val_accuracy: 0.9128\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 438s 292ms/step - loss: 0.2191 - accuracy: 0.9310 - val_loss: 0.2534 - val_accuracy: 0.9172\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 485s 324ms/step - loss: 0.1585 - accuracy: 0.9479 - val_loss: 0.2764 - val_accuracy: 0.9153\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 474s 316ms/step - loss: 0.1152 - accuracy: 0.9607 - val_loss: 0.3427 - val_accuracy: 0.9125\n",
      "Epoch 5/10\n",
      " 101/1500 [=>............................] - ETA: 6:48 - loss: 0.0646 - accuracy: 0.9769"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ────────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 8) Train\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ────────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystop_cb\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32md:\\Softwares\\anaconda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 8) Train\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]\n",
    ") #96 000 no of training samples / 64 batch size = 1500 batches in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98aae800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 7s 60ms/step - loss: 0.3827 - accuracy: 0.9084\n",
      "Test accuracy: 0.9084\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 9) Evaluate on Test Set\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "009ca524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Text: Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disap …\n",
      "True: Business | Pred: Business (conf 76.20%)\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Text: The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - T …\n",
      "True: Sci/Tech | Pred: Sci/Tech (conf 100.00%)\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Text: Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at th …\n",
      "True: Sci/Tech | Pred: Sci/Tech (conf 99.85%)\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Text: Prediction Unit Helps Forecast Wildfires (AP) AP - It's barely dawn when Mike Fitzpatrick starts his …\n",
      "True: Sci/Tech | Pred: World (conf 50.27%)\n",
      "\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Text: Calif. Aims to Limit Farm-Related Smog (AP) AP - Southern California's smog-fighting agency went aft …\n",
      "True: Sci/Tech | Pred: Sci/Tech (conf 99.93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 10) Demo Predictions\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "for sample_text, true_label in zip(test_texts[:5], test_labels[:5]):\n",
    "    token_ids = vectorizer(tf.constant([sample_text]))\n",
    "    probs     = model.predict(token_ids)[0]\n",
    "    pred      = tf.argmax(probs).numpy()\n",
    "    print(\"Text:\", sample_text[:100], \"…\")\n",
    "    print(f\"True: {CLASS_NAMES[true_label]} | Pred: {CLASS_NAMES[pred]} (conf {probs[pred]:.2%})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8474b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocabulary (20000 tokens) saved to agnews_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# 10) Save the TextVectorization vocabulary\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "with open(\"D:/AIML/data/agnews_vocab.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for token in vocab:\n",
    "        f.write(token + \"\\n\")\n",
    "print(f\"✅ Vocabulary ({len(vocab)} tokens) saved to agnews_vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775cf95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
