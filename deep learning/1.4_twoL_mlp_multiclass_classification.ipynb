{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f104b6",
   "metadata": {},
   "source": [
    "04_mlp_multiclass_classification.ipynb: 2 layers (1 hidden + 1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba4d23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 1.0826 | Train Acc: 0.7979 | Val Acc: 0.7500\n",
      "Epoch  50 | Loss: 0.3472 | Train Acc: 0.9000 | Val Acc: 0.9000\n",
      "Epoch 100 | Loss: 0.2255 | Train Acc: 0.9250 | Val Acc: 0.9167\n",
      "Epoch 150 | Loss: 0.1962 | Train Acc: 0.9271 | Val Acc: 0.9333\n",
      "Epoch 200 | Loss: 0.1854 | Train Acc: 0.9271 | Val Acc: 0.9333\n",
      "\n",
      "Final Train Acc: 0.9271 | Final Val Acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Synthetic Multiclass Dataset\n",
    "#    (reuse from Example 2)\n",
    "# ----------------------------\n",
    "def generate_multiclass_data(n_per_class=200, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class\n",
    "    cov = [[0.3, 0], [0, 0.3]]\n",
    "    x0 = np.random.multivariate_normal(mean=[-1, 0], cov=cov, size=N)\n",
    "    y0 = np.zeros((N, 3)); y0[:, 0] = 1\n",
    "    x1 = np.random.multivariate_normal(mean=[1, 0], cov=cov, size=N)\n",
    "    y1 = np.zeros((N, 3)); y1[:, 1] = 1\n",
    "    x2 = np.random.multivariate_normal(mean=[0, 1.5], cov=cov, size=N)\n",
    "    y2 = np.zeros((N, 3)); y2[:, 2] = 1\n",
    "    X = np.vstack([x0, x1, x2])\n",
    "    Y = np.vstack([y0, y1, y2])\n",
    "    perm = np.random.permutation(3 * N)\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Model: MLP with one hidden layer\n",
    "# ----------------------------\n",
    "class MLPMulti:\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, lr=0.1):\n",
    "        \"\"\"\n",
    "        2‐layer MLP for K‐class classification:\n",
    "          Input → Dense(hidden_dim) → ReLU → Dense(out_dim) → Softmax\n",
    "        \"\"\"\n",
    "        # He initialization for hidden (ReLU)\n",
    "        self.W1 = np.random.randn(in_dim, hidden_dim) * np.sqrt(2 / in_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        # Small random for output\n",
    "        self.W2 = np.random.randn(hidden_dim, out_dim) * 0.01\n",
    "        self.b2 = np.zeros((1, out_dim))\n",
    "        self.lr = lr\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_grad(self, z):\n",
    "        grad = np.zeros_like(z)\n",
    "        grad[z > 0] = 1\n",
    "        return grad\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z_shift = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_shift)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch, 2)\n",
    "        Returns cache and output probabilities (batch, 3).\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        Z1 = X.dot(self.W1) + self.b1     # (batch, hidden_dim)\n",
    "        A1 = self.relu(Z1)                # (batch, hidden_dim)\n",
    "\n",
    "        # Output layer\n",
    "        Z2 = A1.dot(self.W2) + self.b2    # (batch, out_dim)\n",
    "        A2 = self.softmax(Z2)             # (batch, out_dim)\n",
    "\n",
    "        cache = (X, Z1, A1, Z2, A2)\n",
    "        return cache, A2\n",
    "\n",
    "    def compute_loss_and_grad(self, cache, A2, Y_onehot):\n",
    "        \"\"\"\n",
    "        Y_onehot: (batch, out_dim)\n",
    "        Returns: loss, grads dict\n",
    "        \"\"\"\n",
    "        X, Z1, A1, Z2, A2 = cache\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Loss: multiclass cross‐entropy\n",
    "        A2_clipped = np.clip(A2, 1e-8, 1 - 1e-8)\n",
    "        loss = -np.sum(Y_onehot * np.log(A2_clipped)) / m\n",
    "\n",
    "        # Backprop\n",
    "        dZ2 = (A2 - Y_onehot) / m         # (batch, out_dim)\n",
    "        dW2 = A1.T.dot(dZ2)               # (hidden_dim, out_dim)\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)  # (1, out_dim)\n",
    "\n",
    "        dA1 = dZ2.dot(self.W2.T)          # (batch, hidden_dim)\n",
    "        dZ1 = dA1 * self.relu_grad(Z1)    # (batch, hidden_dim)\n",
    "        dW1 = X.T.dot(dZ1)                # (in_dim, hidden_dim)\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)  # (1, hidden_dim)\n",
    "\n",
    "        grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "        return loss, grads\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        self.W1 -= self.lr * grads[\"dW1\"]\n",
    "        self.b1 -= self.lr * grads[\"db1\"]\n",
    "        self.W2 -= self.lr * grads[\"dW2\"]\n",
    "        self.b2 -= self.lr * grads[\"db2\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, A2 = self.forward(X)\n",
    "        return np.argmax(A2, axis=1)  # (batch,)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Training Loop\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    X, Y = generate_multiclass_data(n_per_class=200, seed=0)\n",
    "    Y_int = np.argmax(Y, axis=1)\n",
    "\n",
    "    # Split 80% train, 20% val\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, Y_train = X[:split], Y[:split]\n",
    "    X_val,   Y_val   = X[split:], Y[split:]\n",
    "    Y_val_int = Y_int[split:]\n",
    "\n",
    "    # Instantiate model\n",
    "    model = MLPMulti(in_dim=2, hidden_dim=8, out_dim=3, lr=0.1)\n",
    "    epochs = 200\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        cache, A2 = model.forward(X_train)\n",
    "        loss, grads = model.compute_loss_and_grad(cache, A2, Y_train)\n",
    "        model.update_params(grads)\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            train_preds = model.predict(X_train)\n",
    "            val_preds   = model.predict(X_val)\n",
    "            train_acc = np.mean(train_preds == np.argmax(Y_train, axis=1))\n",
    "            val_acc   = np.mean(val_preds   == Y_val_int)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    train_acc = np.mean(model.predict(X_train) == np.argmax(Y_train, axis=1))\n",
    "    val_acc   = np.mean(model.predict(X_val)   == Y_val_int)\n",
    "    print(f\"\\nFinal Train Acc: {train_acc:.4f} | Final Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skhan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
