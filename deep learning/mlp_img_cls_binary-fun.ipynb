{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b067aead",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras\n",
    "## Building an Image Classifier Using the Functional API\n",
    "### Using Keras to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe8a146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00ef4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load & split the dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f580637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n",
      "(55000,)\n",
      "(28, 28)\n",
      "()\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_train[0].shape)\n",
    "print(y_train[0].shape)\n",
    "\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7898d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Normalize pixels to [0,1]\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_valid = X_valid.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\")  / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cba81cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2, 2))  # Smaller size\n",
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ab7b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57a90578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlp_fashion_mnist\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 28, 28)]          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " normalization (Normalizati  (None, 784)               1569      \n",
      " on)                                                             \n",
      "                                                                 \n",
      " hidden_dense_1 (Dense)      (None, 300)               235500    \n",
      "                                                                 \n",
      " hidden_dense_2 (Dense)      (None, 100)               30100     \n",
      "                                                                 \n",
      " output_probs (Dense)        (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 268179 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 1569 (6.13 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3) Functionalâ€‘API model definition\n",
    "inputs = layers.Input(shape=(28, 28), name=\"input_image\")\n",
    "\n",
    "# -- Flatten image to vector\n",
    "x = layers.Flatten(name=\"flatten\")(inputs)\n",
    "\n",
    "# -- (Optional) apply a normalization layer on the flattened vectors\n",
    "normalizer = layers.Normalization(name=\"normalization\")\n",
    "# Must adapt on flattened training data:\n",
    "normalizer.adapt(X_train.reshape(-1, 28*28))\n",
    "x = normalizer(x)\n",
    "\n",
    "# -- Hidden MLP layers\n",
    "x = layers.Dense(300, activation=\"relu\", name=\"hidden_dense_1\")(x)\n",
    "x = layers.Dense(100, activation=\"relu\", name=\"hidden_dense_2\")(x)\n",
    "\n",
    "# -- Output layer: 10 classes with softmax\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"output_probs\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs, name=\"mlp_fashion_mnist\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4431a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Compile for classification\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "176a05e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5266 - accuracy: 0.8165 - val_loss: 0.4101 - val_accuracy: 0.8468\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3677 - accuracy: 0.8690 - val_loss: 0.3774 - val_accuracy: 0.8594\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3276 - accuracy: 0.8820 - val_loss: 0.3490 - val_accuracy: 0.8738\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3009 - accuracy: 0.8903 - val_loss: 0.3449 - val_accuracy: 0.8770\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2806 - accuracy: 0.8979 - val_loss: 0.3369 - val_accuracy: 0.8766\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2632 - accuracy: 0.9057 - val_loss: 0.3312 - val_accuracy: 0.8792\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2479 - accuracy: 0.9101 - val_loss: 0.3238 - val_accuracy: 0.8814\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2340 - accuracy: 0.9161 - val_loss: 0.3444 - val_accuracy: 0.8776\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2211 - accuracy: 0.9219 - val_loss: 0.3251 - val_accuracy: 0.8804\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2096 - accuracy: 0.9246 - val_loss: 0.3239 - val_accuracy: 0.8838\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1986 - accuracy: 0.9295 - val_loss: 0.3250 - val_accuracy: 0.8866\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1886 - accuracy: 0.9331 - val_loss: 0.3253 - val_accuracy: 0.8884\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1780 - accuracy: 0.9370 - val_loss: 0.3198 - val_accuracy: 0.8870\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1683 - accuracy: 0.9405 - val_loss: 0.3245 - val_accuracy: 0.8886\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1603 - accuracy: 0.9439 - val_loss: 0.3369 - val_accuracy: 0.8852\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1523 - accuracy: 0.9472 - val_loss: 0.3358 - val_accuracy: 0.8852\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1446 - accuracy: 0.9506 - val_loss: 0.3430 - val_accuracy: 0.8836\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1366 - accuracy: 0.9533 - val_loss: 0.3367 - val_accuracy: 0.8860\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1294 - accuracy: 0.9566 - val_loss: 0.3667 - val_accuracy: 0.8804\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1228 - accuracy: 0.9582 - val_loss: 0.3665 - val_accuracy: 0.8782\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1157 - accuracy: 0.9609 - val_loss: 0.3484 - val_accuracy: 0.8880\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1087 - accuracy: 0.9647 - val_loss: 0.3514 - val_accuracy: 0.8924\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1039 - accuracy: 0.9662 - val_loss: 0.3541 - val_accuracy: 0.8882\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0984 - accuracy: 0.9675 - val_loss: 0.3642 - val_accuracy: 0.8862\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0923 - accuracy: 0.9706 - val_loss: 0.3704 - val_accuracy: 0.8890\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0876 - accuracy: 0.9714 - val_loss: 0.3796 - val_accuracy: 0.8870\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0838 - accuracy: 0.9734 - val_loss: 0.3873 - val_accuracy: 0.8890\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0811 - accuracy: 0.9754 - val_loss: 0.3941 - val_accuracy: 0.8862\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0759 - accuracy: 0.9765 - val_loss: 0.3953 - val_accuracy: 0.8866\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0722 - accuracy: 0.9783 - val_loss: 0.4045 - val_accuracy: 0.8870\n"
     ]
    }
   ],
   "source": [
    "# 5) Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f808c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4257 - accuracy: 0.8805\n",
      "\n",
      "Test Accuracy: 0.8805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d58883c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.06 0.   0.   0.   0.   0.   0.94 0.   0.   0.  ]]\n",
      "Sample 0: Predicted=9, True=9\n",
      "Sample 1: Predicted=2, True=2\n",
      "Sample 2: Predicted=1, True=1\n",
      "Sample 3: Predicted=1, True=1\n",
      "Sample 4: Predicted=6, True=6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) Predict on new samples\n",
    "X_new = X_test[:5]\n",
    "y_pred_probs = model.predict(X_new)\n",
    "print(y_pred_probs.round(2))\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "for i, (pred, true) in enumerate(zip(y_pred, y_test[:5])):\n",
    "    print(f\"Sample {i}: Predicted={pred}, True={true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9d26809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, 1, 6], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred_probs.argmax(axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41d39d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser', 'Trouser', 'Shirt'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cfbaafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, 1, 6], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:5]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0004a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
