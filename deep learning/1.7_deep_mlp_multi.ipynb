{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.\tData Generation\n",
    "\t•\tWe create 3 Gaussian clusters in 2D, each labeled with a one‐hot vector of length 3.\n",
    "\t•\tSplit 80% train / 20% validation.\n",
    "2.\tArchitecture (layer_dims = [2,16,16,8,8,3])\n",
    "\t•\tInput: 2\n",
    "\t•\tHidden 1: 16 (ReLU + BatchNorm)\n",
    "\t•\tHidden 2: 16 (ReLU + BatchNorm)\n",
    "\t•\tHidden 3: 8 (ReLU + BatchNorm)\n",
    "\t•\tHidden 4: 8 (ReLU + BatchNorm)\n",
    "\t•\tOutput: 3 (Softmax over 3 classes)\n",
    "3.\tBatch Normalization\n",
    "\t•\tEach hidden layer ℓ (ℓ=1…4) has BatchNorm that normalizes Zℓ before applying ReLU.\n",
    "\t•\tIn forward:\n",
    "    •\tIn backward:\n",
    "\t    1.\tCompute dZ_norm = dAℓ * ReLU′(Zℓ_norm).\n",
    "\t    2.\tThen dZ = BNℓ.backward(dZ_norm) and collect dgammaℓ, dbetaℓ.\n",
    "\t    3.\tFinally backprop through the linear transformation.\n",
    "4.\tInitialization\n",
    "\t•\tHidden layers use He initialization (np.random.randn(...) * sqrt(2/n_in)).\n",
    "\t•\tOutput layer uses small random normal (* 0.01).\n",
    "5.\tOptimizer: Adam\n",
    "\t•\tWe maintain m and v for each parameter (Wℓ, bℓ, γℓ, βℓ).\n",
    "\t•\tBias‐corrected updates per epoch with learning rate 0.001.\n",
    "\n",
    "6.\tLoss & Backprop\n",
    "\t•\tLoss: multiclass cross‐entropy\n",
    "\t•\tOutput grad: dZL = (A_L - Y) / m.\n",
    "\t•\tThen standard chain rule through hidden layers.\n",
    "7.\tTraining Loop\n",
    "\t•\tFor each epoch:\n",
    "\t•\tForward pass\n",
    "\t•\tCompute loss & gradients\n",
    "\t•\tAdam parameter update\n",
    "\t•\tPrint training & validation accuracy every 20 epochs (and epoch 1).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac916e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 1.0983 | Train Acc: 0.4135 | Val Acc: 0.3917\n",
      "Epoch  20 | Loss: 1.0911 | Train Acc: 0.6917 | Val Acc: 0.7000\n",
      "Epoch  40 | Loss: 1.0825 | Train Acc: 0.7281 | Val Acc: 0.7000\n",
      "Epoch  60 | Loss: 1.0702 | Train Acc: 0.7542 | Val Acc: 0.7417\n",
      "Epoch  80 | Loss: 1.0527 | Train Acc: 0.7948 | Val Acc: 0.7833\n",
      "Epoch 100 | Loss: 1.0285 | Train Acc: 0.8260 | Val Acc: 0.8042\n",
      "Epoch 120 | Loss: 0.9984 | Train Acc: 0.8458 | Val Acc: 0.8125\n",
      "Epoch 140 | Loss: 0.9637 | Train Acc: 0.8615 | Val Acc: 0.8250\n",
      "Epoch 160 | Loss: 0.9251 | Train Acc: 0.8760 | Val Acc: 0.8417\n",
      "Epoch 180 | Loss: 0.8834 | Train Acc: 0.8885 | Val Acc: 0.8542\n",
      "Epoch 200 | Loss: 0.8392 | Train Acc: 0.8938 | Val Acc: 0.8625\n",
      "\n",
      "Final Train Acc: 0.8938 | Final Val Acc: 0.8625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1) Synthetic Multiclass Dataset\n",
    "# ----------------------------------------\n",
    "def generate_multiclass_data(n_per_class=400, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a 3‐class dataset in 2D:\n",
    "      - Class 0: Gaussian around (-1, 0)\n",
    "      - Class 1: Gaussian around (1,  0)\n",
    "      - Class 2: Gaussian around (0,  1.5)\n",
    "    Returns:\n",
    "      X: (3*n_per_class, 2)\n",
    "      Y: one‐hot labels shape (3*n_per_class, 3)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class\n",
    "    cov = [[0.4, 0], [0, 0.4]]\n",
    "\n",
    "    x0 = np.random.multivariate_normal(mean=[-1, 0], cov=cov, size=N)\n",
    "    y0 = np.zeros((N, 3));  y0[:, 0] = 1\n",
    "\n",
    "    x1 = np.random.multivariate_normal(mean=[1, 0], cov=cov, size=N)\n",
    "    y1 = np.zeros((N, 3));  y1[:, 1] = 1\n",
    "\n",
    "    x2 = np.random.multivariate_normal(mean=[0, 1.5], cov=cov, size=N)\n",
    "    y2 = np.zeros((N, 3));  y2[:, 2] = 1\n",
    "\n",
    "    X = np.vstack([x0, x1, x2])  # (3N, 2)\n",
    "    Y = np.vstack([y0, y1, y2])  # (3N, 3)\n",
    "\n",
    "    perm = np.random.permutation(3 * N)\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2) Batch Normalization Layer\n",
    "# ----------------------------------------\n",
    "class BatchNorm:\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Learnable scale (γ) and shift (β)\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "\n",
    "        # Running (EMA) statistics for inference\n",
    "        self.running_mean = np.zeros((1, num_features))\n",
    "        self.running_var = np.ones((1, num_features))\n",
    "\n",
    "        # Cache for backprop\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        X: shape (batch, num_features)\n",
    "        Returns: normalized and scaled output, same shape\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            batch_mean = np.mean(X, axis=0, keepdims=True)      # (1, F)\n",
    "            batch_var = np.var(X, axis=0, keepdims=True)        # (1, F)\n",
    "            X_centered = X - batch_mean                         # (batch, F)\n",
    "            inv_std = 1.0 / np.sqrt(batch_var + self.eps)       # (1, F)\n",
    "            X_hat = X_centered * inv_std                        # (batch, F)\n",
    "            out = self.gamma * X_hat + self.beta                 # (batch, F)\n",
    "\n",
    "            # Update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var  = self.momentum * self.running_var  + (1 - self.momentum) * batch_var\n",
    "\n",
    "            # Cache for backward\n",
    "            self.cache = (X_hat, inv_std, X_centered, batch_var, X.shape[0])\n",
    "            return out\n",
    "        else:\n",
    "            X_centered = X - self.running_mean\n",
    "            X_hat = X_centered / np.sqrt(self.running_var + self.eps)\n",
    "            return self.gamma * X_hat + self.beta\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        dout: upstream gradient, shape (batch, num_features)\n",
    "        Returns: dX (same shape), and computes self.dgamma, self.dbeta\n",
    "        \"\"\"\n",
    "        X_hat, inv_std, X_centered, var, m = self.cache\n",
    "\n",
    "        # Gradients for gamma, beta\n",
    "        self.dgamma = np.sum(dout * X_hat, axis=0, keepdims=True)  # (1, F)\n",
    "        self.dbeta  = np.sum(dout, axis=0, keepdims=True)           # (1, F)\n",
    "\n",
    "        # Backprop through normalization\n",
    "        dX_hat = dout * self.gamma                                   # (batch, F)\n",
    "        dvar = np.sum(dX_hat * X_centered * (-0.5) * inv_std**3, axis=0, keepdims=True)  # (1, F)\n",
    "        dmean = (np.sum(dX_hat * -inv_std, axis=0, keepdims=True) +\n",
    "                 dvar * np.mean(-2 * X_centered, axis=0, keepdims=True))                # (1, F)\n",
    "        dX = (dX_hat * inv_std) + (dvar * 2 * X_centered / m) + (dmean / m)               # (batch, F)\n",
    "        return dX\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3) Deep MLP for Multiclass Classification (Corrected)\n",
    "# ----------------------------------------\n",
    "class DeepMLPMulti:\n",
    "    def __init__(self, layer_dims, lr=0.001, seed=0):\n",
    "        \"\"\"\n",
    "        layer_dims: List of sizes, e.g. [2, 16, 16, 8, 8, 3]\n",
    "        (Here: 2 inputs → 4 hidden layers → 3‐class output)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.lr = lr\n",
    "        self.L = len(layer_dims) - 1  # number of dense layers\n",
    "\n",
    "        # Parameters: Wℓ, bℓ, and BatchNorm ℓ for hidden layers (ℓ=1...L-1)\n",
    "        self.params = {}\n",
    "        self.batchnorms = {}\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            n_in = layer_dims[ℓ - 1]\n",
    "            n_out = layer_dims[ℓ]\n",
    "\n",
    "            if ℓ < self.L:\n",
    "                # Hidden layer → He initialization (for ReLU)\n",
    "                self.params[f\"W{ℓ}\"] = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\n",
    "            else:\n",
    "                # Output layer → small random\n",
    "                self.params[f\"W{ℓ}\"] = np.random.randn(n_in, n_out) * 0.01\n",
    "\n",
    "            self.params[f\"b{ℓ}\"] = np.zeros((1, n_out))\n",
    "\n",
    "            # Create BatchNorm only for hidden layers ℓ < L\n",
    "            if ℓ < self.L:\n",
    "                self.batchnorms[f\"BN{ℓ}\"] = BatchNorm(num_features=n_out)\n",
    "\n",
    "        # Adam optimizer state for W, b, gamma, beta\n",
    "        self.adam_m = {}\n",
    "        self.adam_v = {}\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            self.adam_m[f\"W{ℓ}\"] = np.zeros_like(self.params[f\"W{ℓ}\"])\n",
    "            self.adam_v[f\"W{ℓ}\"] = np.zeros_like(self.params[f\"W{ℓ}\"])\n",
    "            self.adam_m[f\"b{ℓ}\"] = np.zeros_like(self.params[f\"b{ℓ}\"])\n",
    "            self.adam_v[f\"b{ℓ}\"] = np.zeros_like(self.params[f\"b{ℓ}\"])\n",
    "        for ℓ in range(1, self.L):\n",
    "            self.adam_m[f\"gamma{ℓ}\"] = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].gamma)\n",
    "            self.adam_v[f\"gamma{ℓ}\"] = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].gamma)\n",
    "            self.adam_m[f\"beta{ℓ}\"]  = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].beta)\n",
    "            self.adam_v[f\"beta{ℓ}\"]  = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].beta)\n",
    "\n",
    "        self.adam_t = 0\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_grad(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"\n",
    "        Z: shape (batch, C)\n",
    "        returns: (batch, C)\n",
    "        \"\"\"\n",
    "        Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\n",
    "        exp_Z = np.exp(Z_shifted)\n",
    "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "        Returns:\n",
    "          A_L: (batch, C) final probabilities\n",
    "          cache: dictionary of all intermediate Zℓ, Znormℓ, Aℓ\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        A_prev = X\n",
    "        cache[\"A0\"] = X\n",
    "\n",
    "        # Hidden layers ℓ = 1 ... L-1\n",
    "        for ℓ in range(1, self.L):\n",
    "            Wℓ = self.params[f\"W{ℓ}\"]\n",
    "            bℓ = self.params[f\"b{ℓ}\"]\n",
    "\n",
    "            # 1) Linear\n",
    "            Zℓ = A_prev.dot(Wℓ) + bℓ                       # (batch, hidden_dim)\n",
    "\n",
    "            # 2) BatchNorm\n",
    "            BNℓ = self.batchnorms[f\"BN{ℓ}\"]\n",
    "            Zℓ_norm = BNℓ.forward(Zℓ, training=training)    # (batch, hidden_dim)\n",
    "\n",
    "            # 3) ReLU\n",
    "            Aℓ = self.relu(Zℓ_norm)                         # (batch, hidden_dim)\n",
    "\n",
    "            cache[f\"Z{ℓ}\"]      = Zℓ\n",
    "            cache[f\"Znorm{ℓ}\"]  = Zℓ_norm\n",
    "            cache[f\"A{ℓ}\"]      = Aℓ\n",
    "            A_prev = Aℓ\n",
    "\n",
    "        # Output layer ℓ = L\n",
    "        Wℓ = self.params[f\"W{self.L}\"]\n",
    "        bℓ = self.params[f\"b{self.L}\"]\n",
    "        Zℓ = A_prev.dot(Wℓ) + bℓ                           # (batch, num_classes)\n",
    "        Aℓ = self.softmax(Zℓ)                              # (batch, num_classes)\n",
    "\n",
    "        cache[f\"Z{self.L}\"] = Zℓ\n",
    "        cache[f\"A{self.L}\"] = Aℓ\n",
    "\n",
    "        return Aℓ, cache\n",
    "\n",
    "    def compute_loss_and_grads(self, A_L, Y, cache):\n",
    "        \"\"\"\n",
    "        Compute multiclass cross‐entropy loss and backprop gradients.\n",
    "        A_L: (batch, C), Y: (batch, C) one‐hot\n",
    "        cache: stored intermediates\n",
    "        Returns:\n",
    "          loss (scalar), grads dict containing dWℓ, dbℓ, dgammaℓ, dbetaℓ\n",
    "        \"\"\"\n",
    "        m = Y.shape[0]\n",
    "        # Compute loss\n",
    "        A_L_clipped = np.clip(A_L, 1e-8, 1 - 1e-8)\n",
    "        loss = -np.sum(Y * np.log(A_L_clipped)) / m\n",
    "\n",
    "        grads = {}\n",
    "        # -------------------\n",
    "        # Output layer ℓ = L\n",
    "        # -------------------\n",
    "        ZL = cache[f\"Z{self.L}\"]                     # (batch, C)\n",
    "        A_prev = cache[f\"A{self.L - 1}\"]             # (batch, hidden_dim of L-1)\n",
    "        dZL = (A_L - Y) / m                          # (batch, C)\n",
    "        grads[f\"dW{self.L}\"] = A_prev.T.dot(dZL)      # (hidden_dim, C)\n",
    "        grads[f\"db{self.L}\"] = np.sum(dZL, axis=0, keepdims=True)  # (1, C)\n",
    "        dA_prev = dZL.dot(self.params[f\"W{self.L}\"].T)             # (batch, hidden_dim)\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Hidden layers ℓ = L-1 ... 1 (reverse)\n",
    "        # -----------------------------------\n",
    "        for ℓ in reversed(range(1, self.L)):\n",
    "            Zℓ      = cache[f\"Z{ℓ}\"]                  # (batch, hidden_dim)\n",
    "            Zℓ_norm = cache[f\"Znorm{ℓ}\"]              # (batch, hidden_dim)\n",
    "            A_prev  = cache[f\"A{ℓ - 1}\"]               # (batch, size of layer ℓ-1)\n",
    "\n",
    "            # 1) Backprop through ReLU\n",
    "            dZ_norm = dA_prev * self.relu_grad(Zℓ_norm)  # (batch, hidden_dim)\n",
    "\n",
    "            # 2) Backprop through BatchNorm\n",
    "            BNℓ = self.batchnorms[f\"BN{ℓ}\"]\n",
    "            dZ    = BNℓ.backward(dZ_norm)                # (batch, hidden_dim)\n",
    "            grads[f\"dgamma{ℓ}\"] = BNℓ.dgamma             # (1, hidden_dim)\n",
    "            grads[f\"dbeta{ℓ}\"]  = BNℓ.dbeta              # (1, hidden_dim)\n",
    "\n",
    "            # 3) Backprop through linear\n",
    "            grads[f\"dW{ℓ}\"] = A_prev.T.dot(dZ)            # (layer_dims[ℓ-1], hidden_dim)\n",
    "            grads[f\"db{ℓ}\"] = np.sum(dZ, axis=0, keepdims=True)  # (1, hidden_dim)\n",
    "            dA_prev = dZ.dot(self.params[f\"W{ℓ}\"].T)     # (batch, size of layer ℓ-1)\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def update_params_adam(self, grads):\n",
    "        \"\"\"\n",
    "        Update all parameters with Adam optimizer.\n",
    "        grads: dictionary containing gradients for dWℓ, dbℓ, dgammaℓ, dbetaℓ.\n",
    "        \"\"\"\n",
    "        self.adam_t += 1\n",
    "        lr_t = self.lr * np.sqrt(1 - self.beta2**self.adam_t) / (1 - self.beta1**self.adam_t)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            for param in [f\"W{ℓ}\", f\"b{ℓ}\"]:\n",
    "                grad = grads[f\"d{param}\"]\n",
    "                # Update biased first moment\n",
    "                self.adam_m[param] = self.beta1 * self.adam_m[param] + (1 - self.beta1) * grad\n",
    "                # Update biased second moment\n",
    "                self.adam_v[param] = self.beta2 * self.adam_v[param] + (1 - self.beta2) * (grad ** 2)\n",
    "                # Compute bias‐corrected estimates\n",
    "                m_hat = self.adam_m[param] / (1 - self.beta1**self.adam_t)\n",
    "                v_hat = self.adam_v[param] / (1 - self.beta2**self.adam_t)\n",
    "                # Update parameter\n",
    "                self.params[param] -= lr_t * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "        # Update batchnorm γ and β for ℓ = 1...L-1\n",
    "        for ℓ in range(1, self.L):\n",
    "            for param in [f\"gamma{ℓ}\", f\"beta{ℓ}\"]:\n",
    "                grad_key = \"d\" + param       # e.g. \"dgamma1\" or \"dbeta1\"\n",
    "                grad = grads[grad_key]\n",
    "                self.adam_m[param] = self.beta1 * self.adam_m[param] + (1 - self.beta1) * grad\n",
    "                self.adam_v[param] = self.beta2 * self.adam_v[param] + (1 - self.beta2) * (grad ** 2)\n",
    "                m_hat = self.adam_m[param] / (1 - self.beta1**self.adam_t)\n",
    "                v_hat = self.adam_v[param] / (1 - self.beta2**self.adam_t)\n",
    "                if param.startswith(\"gamma\"):\n",
    "                    ℓ_idx = int(param.replace(\"gamma\", \"\"))\n",
    "                    self.batchnorms[f\"BN{ℓ_idx}\"].gamma -= lr_t * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "                else:  # \"beta\"\n",
    "                    ℓ_idx = int(param.replace(\"beta\", \"\"))\n",
    "                    self.batchnorms[f\"BN{ℓ_idx}\"].beta -= lr_t * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def predict(self, X):\n",
    "        A_L, _ = self.forward(X, training=False)\n",
    "        return np.argmax(A_L, axis=1)  # (batch,)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4) Training Loop for Deep MLP Multiclass\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    X, Y = generate_multiclass_data(n_per_class=400, seed=42)\n",
    "    Y_int = np.argmax(Y, axis=1)  # integer labels for accuracy\n",
    "\n",
    "    # Split 80% train, 20% validation\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, Y_train = X[:split], Y[:split]\n",
    "    X_val,   Y_val   = X[split:], Y[split:]\n",
    "    Y_val_int        = Y_int[split:]\n",
    "\n",
    "    # Define network dimensions: 2→16→16→8→8→3 (4 hidden layers, 3‐class output)\n",
    "    layer_dims = [2, 16, 16, 8, 8, 3]\n",
    "    model = DeepMLPMulti(layer_dims, lr=0.001, seed=1)\n",
    "\n",
    "    epochs = 200\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Forward + compute loss + backward\n",
    "        A_L, cache = model.forward(X_train, training=True)\n",
    "        loss, grads = model.compute_loss_and_grads(A_L, Y_train, cache)\n",
    "        model.update_params_adam(grads)\n",
    "\n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            train_preds = model.predict(X_train)\n",
    "            val_preds   = model.predict(X_val)\n",
    "            train_acc = np.mean(train_preds == np.argmax(Y_train, axis=1))\n",
    "            val_acc   = np.mean(val_preds   == Y_val_int)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    train_acc = np.mean(model.predict(X_train) == np.argmax(Y_train, axis=1))\n",
    "    val_acc   = np.mean(model.predict(X_val)   == Y_val_int)\n",
    "    print(f\"\\nFinal Train Acc: {train_acc:.4f} | Final Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skhan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
