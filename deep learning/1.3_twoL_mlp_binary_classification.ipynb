{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae1d3b1",
   "metadata": {},
   "source": [
    "03_mlp_binary_classification.ipynb: 2 layers (1 hidden + 1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c70e166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 0.6959 | Train Acc: 0.9594 | Val Acc: 0.9625\n",
      "Epoch  50 | Loss: 0.1855 | Train Acc: 0.9563 | Val Acc: 0.9750\n",
      "Epoch 100 | Loss: 0.1338 | Train Acc: 0.9625 | Val Acc: 0.9750\n",
      "Epoch 150 | Loss: 0.1136 | Train Acc: 0.9625 | Val Acc: 0.9750\n",
      "Epoch 200 | Loss: 0.1032 | Train Acc: 0.9625 | Val Acc: 0.9750\n",
      "\n",
      "Final Train Acc: 0.9625 | Final Val Acc: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Synthetic Binary Dataset\n",
    "#    (reuse from Example 1)\n",
    "# ----------------------------\n",
    "def generate_binary_data(n_per_class=200, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class\n",
    "    cov = [[0.5, 0], [0, 0.5]]\n",
    "    x0 = np.random.multivariate_normal(mean=[-1, -1], cov=cov, size=N)\n",
    "    y0 = np.zeros((N, 1))\n",
    "    x1 = np.random.multivariate_normal(mean=[1, 1], cov=cov, size=N)\n",
    "    y1 = np.ones((N, 1))\n",
    "    X = np.vstack([x0, x1])\n",
    "    Y = np.vstack([y0, y1])\n",
    "    perm = np.random.permutation(2 * N)\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Model: MLP with one hidden layer\n",
    "# ----------------------------\n",
    "class MLPBinary:\n",
    "    def __init__(self, in_dim, hidden_dim, lr=0.1):\n",
    "        \"\"\"\n",
    "        A 2‐layer MLP for binary classification:\n",
    "          Input → Dense(hidden_dim) → ReLU → Dense(1) → Sigmoid\n",
    "        \"\"\"\n",
    "        # Weights & biases\n",
    "        self.W1 = np.random.randn(in_dim, hidden_dim) * np.sqrt(2 / in_dim)  # He init for ReLU\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, 1) * 0.01  # small random\n",
    "        self.b2 = 0.0\n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_grad(self, a):\n",
    "        # a = sigmoid(z)\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_grad(self, z):\n",
    "        grad = np.zeros_like(z)\n",
    "        grad[z > 0] = 1\n",
    "        return grad\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch, 2)\n",
    "        Returns a tuple of caches and final output a2 (batch,1).\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        Z1 = X.dot(self.W1) + self.b1       # (batch, hidden_dim)\n",
    "        A1 = self.relu(Z1)                  # (batch, hidden_dim)\n",
    "\n",
    "        # Layer 2 (output)\n",
    "        Z2 = A1.dot(self.W2) + self.b2      # (batch,1)\n",
    "        A2 = self.sigmoid(Z2)               # (batch,1)\n",
    "\n",
    "        cache = (X, Z1, A1, Z2, A2)\n",
    "        return cache, A2\n",
    "\n",
    "    def compute_loss_and_grad(self, cache, A2, Y):\n",
    "        \"\"\"\n",
    "        cache: (X, Z1, A1, Z2, A2), Y: (batch,1)\n",
    "        Returns: loss, grads dict\n",
    "        \"\"\"\n",
    "        X, Z1, A1, Z2, A2 = cache\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Loss: binary cross‐entropy\n",
    "        A2_clipped = np.clip(A2, 1e-8, 1 - 1e-8)\n",
    "        loss = -np.sum(Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped)) / m\n",
    "\n",
    "        # Backprop\n",
    "        dZ2 = A2 - Y                      # (batch,1)\n",
    "        dW2 = A1.T.dot(dZ2) / m           # (hidden_dim,1)\n",
    "        db2 = np.sum(dZ2) / m             # scalar\n",
    "\n",
    "        dA1 = dZ2.dot(self.W2.T)          # (batch, hidden_dim)\n",
    "        dZ1 = dA1 * self.relu_grad(Z1)    # (batch, hidden_dim)\n",
    "        dW1 = X.T.dot(dZ1) / m            # (2, hidden_dim)\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # (1, hidden_dim)\n",
    "\n",
    "        grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "        return loss, grads\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        self.W1 -= self.lr * grads[\"dW1\"]\n",
    "        self.b1 -= self.lr * grads[\"db1\"]\n",
    "        self.W2 -= self.lr * grads[\"dW2\"]\n",
    "        self.b2 -= self.lr * grads[\"db2\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, A2 = self.forward(X)\n",
    "        return (A2 > 0.5).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Training Loop\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    X, Y = generate_binary_data(n_per_class=200, seed=0)\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, Y_train = X[:split], Y[:split]\n",
    "    X_val,   Y_val   = X[split:], Y[split:]\n",
    "\n",
    "    # Instantiate model\n",
    "    model = MLPBinary(in_dim=2, hidden_dim=8, lr=0.05)\n",
    "    epochs = 200\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        cache, A2 = model.forward(X_train)\n",
    "        loss, grads = model.compute_loss_and_grad(cache, A2, Y_train)\n",
    "        model.update_params(grads)\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            train_preds = model.predict(X_train)\n",
    "            val_preds   = model.predict(X_val)\n",
    "            train_acc = np.mean(train_preds == Y_train)\n",
    "            val_acc   = np.mean(val_preds   == Y_val)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    train_acc = np.mean(model.predict(X_train) == Y_train)\n",
    "    val_acc   = np.mean(model.predict(X_val)   == Y_val)\n",
    "    print(f\"\\nFinal Train Acc: {train_acc:.4f} | Final Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skhan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
