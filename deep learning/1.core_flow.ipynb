{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7075e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "1. Data prep & splitting:\n",
    "    1. Load raw data -> clean/augment\n",
    "    2. Split into train/val/test\n",
    "    3. Scale/normalize features\n",
    "\n",
    "2. Model Definition:\n",
    "    1.Neuron (Weights & Bias)(z=w.x+b)\n",
    "    2. Activation Functions\n",
    "        1.\tLinear (identity):f(z) = z\n",
    "        2.\tSigmoid:f(z) = \\frac{1}{1 + e^{-z}}\n",
    "        3.\tTanh:f(z) = \\tanh(z)\n",
    "        4.\tReLU:f(z) = \\max(0,\\,z)\n",
    "        5.\tBinary Step (Perceptron):f(z) = \\begin{cases}1,&z>0\\\\0,&z\\le0\\end{cases}\n",
    "        6.\tELU:f(z) = \\begin{cases}z,&z>0\\\\\\alpha(e^{z}-1),&z\\le0\\end{cases}\n",
    "    3. Weight Initialization\n",
    "        1.\tXavier (Glorot) Initialization (Sigmoid/Tanh)\n",
    "        2.  He (Kaiming) Initialization (Relu/ELU)\n",
    "    4. Perceptron (Single-Layer Binary Classifier)\n",
    "        A neuron + binary-step activation\n",
    "    5. Feedforward Pass (Multi-Layer)\n",
    "        Stacking Neurons into Layers\n",
    "    6. Softmax (as Output Layer for K-Class)\n",
    "3. Loss / Cost Functions\n",
    "    1. Regression(MSE/Huber)\n",
    "    2. Binary(Log loss/Binary)\n",
    "    3. Multiclass (softmax+cross entropy)\n",
    "4. Backpropagation (Chain Rule)\n",
    "5. Optimizers\n",
    "    1.\tGradient Descent (Batch GD)\n",
    "    2.\tStochastic Gradient Descent (SGD)\n",
    "    3.\tMomentum (optional extension)\n",
    "    4.\tRMSProp\n",
    "    5.\tAdam\n",
    "    6.\tAdagrad\n",
    "    7.\tAdadelta\n",
    "6. Regularization\n",
    "    1.  L2 (Weight Decay)\n",
    "    2.\tL1 (Lasso)\n",
    "    3.\tDropout\n",
    "    4.\tEarly Stopping\n",
    "    5.\tLearning Rate Decay\n",
    "    6.\tGradient Clipping\n",
    "    7.\tWeight Decay (L2)\n",
    "    8.\tLocal vs Global Minimum\n",
    "7. Training Loop\n",
    "    •\tFor each epoch:\n",
    "        •\tShuffle and iterate over mini‐batches\n",
    "        •\tForward → compute loss + metrics → backward → optimizer.step()\n",
    "        •\tLog training/validation loss and metrics\n",
    "    •\tCheckpoint best model (early stop if val loss increases)\n",
    "8.\tEvaluation on Test Set\n",
    "\t•\tAfter training, use the saved “best” checkpoint to report final metrics on an unseen test split.\n",
    "9. Metric Learning (Advanced / Optional)\n",
    "    1.  One‐Shot Learning (Siamese Networks)\n",
    "    2.\tTriplet Loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7c600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
