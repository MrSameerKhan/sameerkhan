{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe2a0c9",
   "metadata": {},
   "source": [
    "02_softmax_regression_multiclass.ipynb: 1 layer (input → output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c7dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 1.0986 | Train Acc: 0.8917 | Val Acc: 0.8750\n",
      "Epoch  50 | Loss: 0.4417 | Train Acc: 0.9167 | Val Acc: 0.9250\n",
      "Epoch 100 | Loss: 0.3257 | Train Acc: 0.9313 | Val Acc: 0.9333\n",
      "Epoch 150 | Loss: 0.2785 | Train Acc: 0.9292 | Val Acc: 0.9417\n",
      "Epoch 200 | Loss: 0.2526 | Train Acc: 0.9250 | Val Acc: 0.9417\n",
      "\n",
      "Final Train Acc: 0.9250 | Final Val Acc: 0.9417\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Synthetic Multiclass Dataset\n",
    "# ----------------------------\n",
    "def generate_multiclass_data(n_per_class=200, seed=0):\n",
    "    \"\"\"\n",
    "    Three 2D Gaussian blobs for classes 0, 1, 2.\n",
    "    Returns:\n",
    "      X: shape (3*n_per_class, 2)\n",
    "      Y: shape (3*n_per_class, 3) one‐hot\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class\n",
    "    cov = [[0.3, 0], [0, 0.3]]\n",
    "\n",
    "    # Class 0: center at (-1, 0)\n",
    "    x0 = np.random.multivariate_normal(mean=[-1, 0], cov=cov, size=N)\n",
    "    y0 = np.zeros((N, 3)); y0[:, 0] = 1\n",
    "\n",
    "    # Class 1: center at (1, 0)\n",
    "    x1 = np.random.multivariate_normal(mean=[1, 0], cov=cov, size=N)\n",
    "    y1 = np.zeros((N, 3)); y1[:, 1] = 1\n",
    "\n",
    "    # Class 2: center at (0, 1.5)\n",
    "    x2 = np.random.multivariate_normal(mean=[0, 1.5], cov=cov, size=N)\n",
    "    y2 = np.zeros((N, 3)); y2[:, 2] = 1\n",
    "\n",
    "    X = np.vstack([x0, x1, x2])  # (3N, 2)\n",
    "    Y = np.vstack([y0, y1, y2])  # (3N, 3)\n",
    "\n",
    "    perm = np.random.permutation(3 * N)\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Model: Softmax Regression\n",
    "# ----------------------------\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, in_dim, num_classes, lr=0.1):\n",
    "        self.W = np.zeros((in_dim, num_classes))  # (2, 3)\n",
    "        self.b = np.zeros((1, num_classes))       # (1, 3)\n",
    "        self.lr = lr\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        z: shape (batch, num_classes)\n",
    "        returns: shape (batch, num_classes)\n",
    "        \"\"\"\n",
    "        z_shift = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_shift)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: shape (batch, 2)\n",
    "        returns: shape (batch, 3), probability distribution\n",
    "        \"\"\"\n",
    "        z = X.dot(self.W) + self.b  # (batch, 3)\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def compute_loss_and_grad(self, X, Y_onehot):\n",
    "        \"\"\"\n",
    "        Y_onehot: (batch, 3)\n",
    "        Returns:\n",
    "          loss: scalar (average CE)\n",
    "          dW: shape (2,3)\n",
    "          db: shape (1,3)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        P = self.forward(X)  # (batch,3)\n",
    "        P_clipped = np.clip(P, 1e-8, 1 - 1e-8)\n",
    "        # Cross‐entropy loss\n",
    "        loss = -np.sum(Y_onehot * np.log(P_clipped)) / m\n",
    "\n",
    "        # Gradient of softmax+CE: dZ = (P - Y)/m\n",
    "        dZ = (P - Y_onehot) / m      # (batch,3)\n",
    "        dW = X.T.dot(dZ)             # (2,3)\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)  # (1,3)\n",
    "        return loss, dW, db\n",
    "\n",
    "    def update_params(self, dW, db):\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        P = self.forward(X)           # (batch,3)\n",
    "        return np.argmax(P, axis=1)   # (batch,)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Training Loop\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    X, Y = generate_multiclass_data(n_per_class=200, seed=0)\n",
    "    # One-hot labels in Y already. Also prepare integer labels for accuracy.\n",
    "    Y_int = np.argmax(Y, axis=1)\n",
    "\n",
    "    # Split 80% train, 20% val\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, Y_train = X[:split], Y[:split]\n",
    "    X_val,   Y_val   = X[split:], Y[split:]\n",
    "    Y_val_int = Y_int[split:]\n",
    "\n",
    "    # Instantiate model\n",
    "    model = SoftmaxRegression(in_dim=2, num_classes=3, lr=0.1)\n",
    "    epochs = 200\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss, dW, db = model.compute_loss_and_grad(X_train, Y_train)\n",
    "        model.update_params(dW, db)\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            # Compute train & val accuracy\n",
    "            train_preds = model.predict(X_train)\n",
    "            val_preds   = model.predict(X_val)\n",
    "            train_acc = np.mean(train_preds == np.argmax(Y_train, axis=1))\n",
    "            val_acc   = np.mean(val_preds   == Y_val_int)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    train_acc = np.mean(model.predict(X_train) == np.argmax(Y_train, axis=1))\n",
    "    val_acc   = np.mean(model.predict(X_val)   == Y_val_int)\n",
    "    print(f\"\\nFinal Train Acc: {train_acc:.4f} | Final Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skhan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
