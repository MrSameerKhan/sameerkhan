{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115ea09a",
   "metadata": {},
   "source": [
    "01_logistic_regression_binary.ipynb: 1 layer (input → output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdd5f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 0.6931 | Train Acc: 0.9656 | Val Acc: 0.9750\n",
      "Epoch  50 | Loss: 0.2041 | Train Acc: 0.9625 | Val Acc: 0.9750\n",
      "Epoch 100 | Loss: 0.1496 | Train Acc: 0.9656 | Val Acc: 0.9750\n",
      "Epoch 150 | Loss: 0.1277 | Train Acc: 0.9656 | Val Acc: 0.9750\n",
      "Epoch 200 | Loss: 0.1156 | Train Acc: 0.9656 | Val Acc: 0.9750\n",
      "\n",
      "Final Train Acc: 0.9656 | Final Val Acc: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Synthetic Binary Dataset\n",
    "# ----------------------------\n",
    "def generate_binary_data(n_per_class=200, seed=0):\n",
    "    \"\"\"\n",
    "    Two Gaussian blobs in 2D for class 0 and class 1.\n",
    "    Returns:\n",
    "      X: shape (2*n_per_class, 2)\n",
    "      Y: shape (2*n_per_class, 1) with values {0,1}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class\n",
    "    cov = [[0.5, 0], [0, 0.5]]\n",
    "\n",
    "    # Class 0 centered at (-1, -1)\n",
    "    x0 = np.random.multivariate_normal(mean=[-1, -1], cov=cov, size=N)\n",
    "    y0 = np.zeros((N, 1))\n",
    "\n",
    "    # Class 1 centered at (+1, +1)\n",
    "    x1 = np.random.multivariate_normal(mean=[1, 1], cov=cov, size=N)\n",
    "    y1 = np.ones((N, 1))\n",
    "\n",
    "    X = np.vstack([x0, x1])  # (2N, 2)\n",
    "    Y = np.vstack([y0, y1])  # (2N, 1)\n",
    "\n",
    "    # Shuffle\n",
    "    perm = np.random.permutation(2 * N)\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Model: Logistic Regression\n",
    "# ----------------------------\n",
    "class LogisticRegression:\n",
    "    def __init__(self, in_dim, lr=0.1):\n",
    "        self.W = np.zeros((in_dim, 1))  # shape (2,1)\n",
    "        self.b = 0.0\n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: shape (batch, 2)\n",
    "        returns: shape (batch, 1), probabilities\n",
    "        \"\"\"\n",
    "        z = X.dot(self.W) + self.b      # (batch,1)\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def compute_loss_and_grad(self, X, Y):\n",
    "        \"\"\"\n",
    "        Binary cross‐entropy loss and gradients\n",
    "        X: (batch,2), Y: (batch,1)\n",
    "        Returns:\n",
    "          loss: scalar\n",
    "          dW: shape (2,1)\n",
    "          db: scalar\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        # Forward\n",
    "        A = self.forward(X)  # (batch,1)\n",
    "        # Clip for numerical stability\n",
    "        A_clipped = np.clip(A, 1e-8, 1 - 1e-8)\n",
    "        # Loss\n",
    "        loss = -np.sum(Y * np.log(A_clipped) + (1 - Y) * np.log(1 - A_clipped)) / m\n",
    "\n",
    "        # Gradients\n",
    "        dZ = A - Y                    # (batch,1)\n",
    "        dW = (X.T.dot(dZ)) / m        # (2,1)\n",
    "        db = np.sum(dZ) / m           # scalar\n",
    "        return loss, dW, db\n",
    "\n",
    "    def update_params(self, dW, db):\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Return {0,1} predictions\n",
    "        \"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return (probs > 0.5).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Training Loop\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    X, Y = generate_binary_data(n_per_class=200, seed=0)\n",
    "    # Split 80% train, 20% val\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, Y_train = X[:split], Y[:split]\n",
    "    X_val,   Y_val   = X[split:], Y[split:]\n",
    "\n",
    "    # Instantiate model\n",
    "    model = LogisticRegression(in_dim=2, lr=0.1)\n",
    "    epochs = 200\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss, dW, db = model.compute_loss_and_grad(X_train, Y_train)\n",
    "        model.update_params(dW, db)\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            # Compute train & val accuracy\n",
    "            train_preds = model.predict(X_train)\n",
    "            val_preds   = model.predict(X_val)\n",
    "            train_acc = np.mean(train_preds == Y_train)\n",
    "            val_acc   = np.mean(val_preds   == Y_val)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    train_acc = np.mean(model.predict(X_train) == Y_train)\n",
    "    val_acc   = np.mean(model.predict(X_val)   == Y_val)\n",
    "    print(f\"\\nFinal Train Acc: {train_acc:.4f} | Final Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skhan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
