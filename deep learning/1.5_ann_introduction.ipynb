{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766d41ec",
   "metadata": {},
   "source": [
    "The example MLP defined layer_dims = [2, 8, 4, 1], which corresponds to:\n",
    "\t•\tLayer 1:  (input 2 → hidden 8)\n",
    "\t•\tLayer 2:  (hidden 8 → hidden 4)\n",
    "\t•\tLayer 3:  (hidden 4 → output 1)\n",
    "\n",
    "So there are 3 trainable (dense) layers in that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a97dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 0.6976 | Train ∕ Val Acc: 0.5458 ∕ 0.5917\n",
      "Epoch  50 | Loss: 0.3189 | Train ∕ Val Acc: 0.9771 ∕ 0.9750\n",
      "Epoch 100 | Loss: 0.2427 | Train ∕ Val Acc: 0.9771 ∕ 0.9750\n",
      "Epoch 150 | Loss: 0.1976 | Train ∕ Val Acc: 0.9771 ∕ 0.9750\n",
      "Epoch 200 | Loss: 0.1684 | Train ∕ Val Acc: 0.9771 ∕ 0.9750\n",
      "\n",
      "Final Train Acc: 0.9771 | Final Val Acc: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Synthetic Binary Dataset\n",
    "# ----------------------------\n",
    "def generate_binary_data(n_per_class=300, seed=1):\n",
    "    \"\"\"\n",
    "    Two Gaussian blobs for classes 0/1 in 2D.\n",
    "    Returns:\n",
    "      X: (2n, 2), Y: (2n,1) with values {0,1}.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class\n",
    "    cov = [[0.5, 0], [0, 0.5]]\n",
    "\n",
    "    x0 = np.random.multivariate_normal(mean=[-1, -1], cov=cov, size=N)\n",
    "    y0 = np.zeros((N, 1))\n",
    "\n",
    "    x1 = np.random.multivariate_normal(mean=[1, 1], cov=cov, size=N)\n",
    "    y1 = np.ones((N, 1))\n",
    "\n",
    "    X = np.vstack([x0, x1])\n",
    "    Y = np.vstack([y0, y1])\n",
    "\n",
    "    perm = np.random.permutation(2 * N)\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) MLP Class Definition\n",
    "# ----------------------------\n",
    "class MLPBinary:\n",
    "    def __init__(self, layer_dims, lr=0.01, seed=0):\n",
    "        \"\"\"\n",
    "        layer_dims: list of dims, e.g. [2, 8, 4, 1]\n",
    "          -> input_dim=2, hidden1=8, hidden2=4, output=1.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.lr = lr\n",
    "        self.L = len(layer_dims) - 1    # number of layers\n",
    "\n",
    "        # Initialize weights & biases\n",
    "        self.params = {}\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            n_in = layer_dims[ℓ-1]\n",
    "            n_out = layer_dims[ℓ]\n",
    "            if ℓ < self.L:\n",
    "                # Hidden: He init (ReLU)\n",
    "                self.params[f\"W{ℓ}\"] = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\n",
    "            else:\n",
    "                # Output: small random\n",
    "                self.params[f\"W{ℓ}\"] = np.random.randn(n_in, n_out) * 0.01\n",
    "            self.params[f\"b{ℓ}\"] = np.zeros((1, n_out))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_grad(self, Z):\n",
    "        dZ = np.zeros_like(Z)\n",
    "        dZ[Z > 0] = 1\n",
    "        return dZ\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward pass through L layers.\n",
    "        Cache Zℓ and Aℓ for each ℓ.\n",
    "        Returns:\n",
    "          A_L: final activation (batch,1)\n",
    "          cache: {\"Z1\":..., \"A1\":..., ..., \"ZL\":..., \"A0\":X}\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        A_prev = X\n",
    "        cache[\"A0\"] = X\n",
    "\n",
    "        # Hidden layers 1 to L-1\n",
    "        for ℓ in range(1, self.L):\n",
    "            Wℓ = self.params[f\"W{ℓ}\"]\n",
    "            bℓ = self.params[f\"b{ℓ}\"]\n",
    "            Zℓ = A_prev.dot(Wℓ) + bℓ              # shape (batch, hidden_dim)\n",
    "            Aℓ = self.relu(Zℓ)                    # ReLU\n",
    "            cache[f\"Z{ℓ}\"] = Zℓ\n",
    "            cache[f\"A{ℓ}\"] = Aℓ\n",
    "            A_prev = Aℓ\n",
    "\n",
    "        # Output layer ℓ = L\n",
    "        Wℓ = self.params[f\"W{self.L}\"]\n",
    "        bℓ = self.params[f\"b{self.L}\"]\n",
    "        Zℓ = A_prev.dot(Wℓ) + bℓ                  # shape (batch, 1)\n",
    "        Aℓ = self.sigmoid(Zℓ)                     # Sigmoid\n",
    "        cache[f\"Z{self.L}\"] = Zℓ\n",
    "        cache[f\"A{self.L}\"] = Aℓ\n",
    "\n",
    "        return Aℓ, cache\n",
    "\n",
    "    def compute_loss_and_grad(self, AL, Y, cache):\n",
    "        \"\"\"\n",
    "        Binary cross‐entropy loss and backprop gradients.\n",
    "        AL: predicted (batch,1), Y: true (batch,1).\n",
    "        cache: from forward.\n",
    "        Returns:\n",
    "          loss: scalar\n",
    "          grads: dictionary of gradients dWℓ, dbℓ for ℓ=1…L\n",
    "        \"\"\"\n",
    "        m = Y.shape[0]\n",
    "        # Compute loss\n",
    "        AL_clipped = np.clip(AL, 1e-8, 1 - 1e-8)\n",
    "        loss = -np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)) / m\n",
    "\n",
    "        grads = {}\n",
    "        # dA_L = -(Y/AL) + [(1-Y)/(1-AL)] but simplifies to AL - Y for BCE+sigmoid\n",
    "        dZL = AL - Y                       # shape (m,1)\n",
    "        A_prev = cache[f\"A{self.L-1}\"]     # shape (m, hidden_dim)\n",
    "        grads[f\"dW{self.L}\"] = A_prev.T.dot(dZL) / m   # (hidden_dim, 1)\n",
    "        grads[f\"db{self.L}\"] = np.sum(dZL, axis=0, keepdims=True) / m  # (1,1)\n",
    "\n",
    "        # Backprop into hidden layers ℓ = L-1 … 1\n",
    "        dA_prev = dZL.dot(self.params[f\"W{self.L}\"].T)  # (m, hidden_dim)\n",
    "\n",
    "        for ℓ in reversed(range(1, self.L)):\n",
    "            Zℓ = cache[f\"Z{ℓ}\"]                     # (m, hidden_dim)\n",
    "            dZℓ = dA_prev * self.relu_grad(Zℓ)      # (m, hidden_dim)\n",
    "            A_prev = cache[f\"A{ℓ-1}\"]               # ℓ-1=0 means X (m, D)\n",
    "            grads[f\"dW{ℓ}\"] = A_prev.T.dot(dZℓ) / m  # (layer_dims[ℓ-1], layer_dims[ℓ])\n",
    "            grads[f\"db{ℓ}\"] = np.sum(dZℓ, axis=0, keepdims=True) / m  # (1, hidden_dim)\n",
    "            if ℓ > 1:\n",
    "                dA_prev = dZℓ.dot(self.params[f\"W{ℓ}\"].T)\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            self.params[f\"W{ℓ}\"] -= self.lr * grads[f\"dW{ℓ}\"]\n",
    "            self.params[f\"b{ℓ}\"] -= self.lr * grads[f\"db{ℓ}\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        AL, _ = self.forward(X)\n",
    "        return (AL > 0.5).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Training Loop\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    X, Y = generate_binary_data(n_per_class=300, seed=1)\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, Y_train = X[:split], Y[:split]\n",
    "    X_val,   Y_val   = X[split:], Y[split:]\n",
    "\n",
    "    # Network dims: [input=2, hidden1=8, hidden2=4, output=1]\n",
    "    layer_dims = [2, 8, 4, 1]\n",
    "    model = MLPBinary(layer_dims, lr=0.05, seed=2)\n",
    "    epochs = 200\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        AL, cache = model.forward(X_train)\n",
    "        loss, grads = model.compute_loss_and_grad(AL, Y_train, cache)\n",
    "        model.update_params(grads)\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            train_preds = model.predict(X_train)\n",
    "            val_preds   = model.predict(X_val)\n",
    "            train_acc = np.mean(train_preds == Y_train)\n",
    "            val_acc   = np.mean(val_preds   == Y_val)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Train ∕ Val Acc: {train_acc:.4f} ∕ {val_acc:.4f}\")\n",
    "\n",
    "    # Final\n",
    "    train_acc = np.mean(model.predict(X_train) == Y_train)\n",
    "    val_acc   = np.mean(model.predict(X_val)   == Y_val)\n",
    "    print(f\"\\nFinal Train Acc: {train_acc:.4f} | Final Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skhan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
