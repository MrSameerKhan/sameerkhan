{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1160bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.\\tArchitecture (layer_dims = [2,16,16,8,8,1])\\n\\t•\\tInput layer: size 2\\n\\t•\\tHidden 1: size 16 (ReLU + BatchNorm)\\n\\t•\\tHidden 2: size 16 (ReLU + BatchNorm)\\n\\t•\\tHidden 3: size 8  (ReLU + BatchNorm)\\n\\t•\\tHidden 4: size 8  (ReLU + BatchNorm)\\n\\t•\\tOutput: size 1 (Sigmoid)\\n2.\\tBatch Normalization\\n\\t•\\tEach hidden layer ℓ (ℓ=1…4) has a BatchNorm instance.\\n3.\\tInitialization\\n\\t•\\tHidden layers use He initialization (np.random.randn(...) * sqrt(2/n_in)).\\n\\t•\\tOutput layer uses small random normal (* 0.01).\\n4.\\tOptimizer: Adam\\n\\t•\\tWe keep m and v for each parameter (Wℓ, bℓ, γℓ, βℓ) and perform bias‐corrected updates each epoch.\\n\\t•\\tLearning rate lr=0.001.\\n5.\\tTraining Loop\\n\\t•\\tFor each epoch:\\n\\t•\\tForward pass (A_L, cache = model.forward(X_train))\\n\\t•\\tCompute loss & grads (loss, grads = model.compute_loss_and_grads(...))\\n\\t•\\tUpdate parameters with Adam (model.update_params_adam(grads))\\n\\t•\\tEvery 20 epochs (and for epoch 1), we print training and validation accuracy.\\n\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.\tArchitecture (layer_dims = [2,16,16,8,8,1])\n",
    "\t•\tInput layer: size 2\n",
    "\t•\tHidden 1: size 16 (ReLU + BatchNorm)\n",
    "\t•\tHidden 2: size 16 (ReLU + BatchNorm)\n",
    "\t•\tHidden 3: size 8  (ReLU + BatchNorm)\n",
    "\t•\tHidden 4: size 8  (ReLU + BatchNorm)\n",
    "\t•\tOutput: size 1 (Sigmoid)\n",
    "2.\tBatch Normalization\n",
    "\t•\tEach hidden layer ℓ (ℓ=1…4) has a BatchNorm instance.\n",
    "3.\tInitialization\n",
    "\t•\tHidden layers use He initialization (np.random.randn(...) * sqrt(2/n_in)).\n",
    "\t•\tOutput layer uses small random normal (* 0.01).\n",
    "4.\tOptimizer: Adam\n",
    "\t•\tWe keep m and v for each parameter (Wℓ, bℓ, γℓ, βℓ) and perform bias‐corrected updates each epoch.\n",
    "\t•\tLearning rate lr=0.001.\n",
    "5.\tTraining Loop\n",
    "\t•\tFor each epoch:\n",
    "\t•\tForward pass (A_L, cache = model.forward(X_train))\n",
    "\t•\tCompute loss & grads (loss, grads = model.compute_loss_and_grads(...))\n",
    "\t•\tUpdate parameters with Adam (model.update_params_adam(grads))\n",
    "\t•\tEvery 20 epochs (and for epoch 1), we print training and validation accuracy.\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f750614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 0.7014 | Train Acc: 0.4788 | Val Acc: 0.5350\n",
      "Epoch  20 | Loss: 0.6974 | Train Acc: 0.2725 | Val Acc: 0.2150\n",
      "Epoch  40 | Loss: 0.6942 | Train Acc: 0.4037 | Val Acc: 0.3450\n",
      "Epoch  60 | Loss: 0.6916 | Train Acc: 0.5100 | Val Acc: 0.4600\n",
      "Epoch  80 | Loss: 0.6884 | Train Acc: 0.5100 | Val Acc: 0.4600\n",
      "Epoch 100 | Loss: 0.6826 | Train Acc: 0.5975 | Val Acc: 0.5600\n",
      "Epoch 120 | Loss: 0.6728 | Train Acc: 0.7550 | Val Acc: 0.7550\n",
      "Epoch 140 | Loss: 0.6588 | Train Acc: 0.8475 | Val Acc: 0.8200\n",
      "Epoch 160 | Loss: 0.6410 | Train Acc: 0.8925 | Val Acc: 0.8550\n",
      "Epoch 180 | Loss: 0.6197 | Train Acc: 0.9325 | Val Acc: 0.8950\n",
      "Epoch 200 | Loss: 0.5958 | Train Acc: 0.9587 | Val Acc: 0.9450\n",
      "\n",
      "Final Train Acc: 0.9587 | Final Val Acc: 0.9450\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1) Synthetic Binary Dataset (reuse)\n",
    "# ----------------------------------------\n",
    "def generate_binary_data(n_per_class=500, seed=42):\n",
    "    \"\"\"\n",
    "    Two Gaussian blobs in 2D for class 0 and class 1.\n",
    "    Returns:\n",
    "      X: shape (2*n_per_class, 2)\n",
    "      Y: shape (2*n_per_class, 1) with values {0,1}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class\n",
    "    cov = [[0.5, 0], [0, 0.5]]\n",
    "\n",
    "    # Class 0 centered at (-1, -1)\n",
    "    x0 = np.random.multivariate_normal(mean=[-1, -1], cov=cov, size=N)\n",
    "    y0 = np.zeros((N, 1))\n",
    "\n",
    "    # Class 1 centered at (+1, +1)\n",
    "    x1 = np.random.multivariate_normal(mean=[1, 1], cov=cov, size=N)\n",
    "    y1 = np.ones((N, 1))\n",
    "\n",
    "    X = np.vstack([x0, x1])  # (2N, 2)\n",
    "    Y = np.vstack([y0, y1])  # (2N, 1)\n",
    "\n",
    "    perm = np.random.permutation(2 * N)\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2) Batch Normalization Layer\n",
    "# ----------------------------------------\n",
    "class BatchNorm:\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Learnable scale (γ) and shift (β)\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "\n",
    "        # Running (EMA) statistics for inference\n",
    "        self.running_mean = np.zeros((1, num_features))\n",
    "        self.running_var = np.ones((1, num_features))\n",
    "\n",
    "        # Cache for backprop\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        X: shape (batch, num_features)\n",
    "        Returns: normalized and scaled output, same shape\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            batch_mean = np.mean(X, axis=0, keepdims=True)       # (1, F)\n",
    "            batch_var = np.var(X, axis=0, keepdims=True)         # (1, F)\n",
    "            X_centered = X - batch_mean                          # (batch, F)\n",
    "            inv_std = 1.0 / np.sqrt(batch_var + self.eps)        # (1, F)\n",
    "            X_hat = X_centered * inv_std                         # (batch, F)\n",
    "            out = self.gamma * X_hat + self.beta                 # (batch, F)\n",
    "\n",
    "            # Update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var  = self.momentum * self.running_var  + (1 - self.momentum) * batch_var\n",
    "\n",
    "            # Cache for backward\n",
    "            self.cache = (X_hat, inv_std, X_centered, batch_var, X.shape[0])\n",
    "            return out\n",
    "        else:\n",
    "            X_centered = X - self.running_mean\n",
    "            X_hat = X_centered / np.sqrt(self.running_var + self.eps)\n",
    "            return self.gamma * X_hat + self.beta\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        dout: upstream gradient, shape (batch, num_features)\n",
    "        Returns: dX (same shape), and sets self.dgamma, self.dbeta\n",
    "        \"\"\"\n",
    "        X_hat, inv_std, X_centered, var, m = self.cache\n",
    "\n",
    "        # Gradients for gamma, beta\n",
    "        self.dgamma = np.sum(dout * X_hat, axis=0, keepdims=True)  # (1, F)\n",
    "        self.dbeta  = np.sum(dout, axis=0, keepdims=True)           # (1, F)\n",
    "\n",
    "        # Backprop through normalization\n",
    "        dX_hat = dout * self.gamma                                   # (batch, F)\n",
    "        dvar = np.sum(dX_hat * X_centered * (-0.5) * inv_std**3, axis=0, keepdims=True)  # (1, F)\n",
    "        dmean = (np.sum(dX_hat * -inv_std, axis=0, keepdims=True) +\n",
    "                 dvar * np.mean(-2 * X_centered, axis=0, keepdims=True))               # (1, F)\n",
    "        dX = (dX_hat * inv_std) + (dvar * 2 * X_centered / m) + (dmean / m)             # (batch, F)\n",
    "        return dX\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3) Deep MLP for Binary Classification (Corrected)\n",
    "# ----------------------------------------\n",
    "class DeepMLP:\n",
    "    def __init__(self, layer_dims, lr=0.001, seed=0):\n",
    "        \"\"\"\n",
    "        layer_dims: list of dims, e.g. [2, 16, 16, 8, 8, 1]\n",
    "        (Here: 2 inputs → 4 hidden layers → 1 output)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.lr = lr\n",
    "        self.L = len(layer_dims) - 1  # number of trainable layers\n",
    "\n",
    "        # Parameters: Wℓ, bℓ, and BatchNorm ℓ for hidden layers (ℓ=1...L-1)\n",
    "        self.params = {}\n",
    "        self.batchnorms = {}\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            n_in = layer_dims[ℓ - 1]\n",
    "            n_out = layer_dims[ℓ]\n",
    "\n",
    "            if ℓ < self.L:\n",
    "                # Hidden layer → He initialization for ReLU\n",
    "                self.params[f\"W{ℓ}\"] = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\n",
    "            else:\n",
    "                # Output layer → small random\n",
    "                self.params[f\"W{ℓ}\"] = np.random.randn(n_in, n_out) * 0.01\n",
    "\n",
    "            self.params[f\"b{ℓ}\"] = np.zeros((1, n_out))\n",
    "\n",
    "            # For hidden layers ℓ < L, create BatchNorm\n",
    "            if ℓ < self.L:\n",
    "                self.batchnorms[f\"BN{ℓ}\"] = BatchNorm(num_features=n_out)\n",
    "\n",
    "        # Adam optimizer state: m, v for Wℓ, bℓ, gammaℓ, betaℓ\n",
    "        self.adam_m = {}\n",
    "        self.adam_v = {}\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            self.adam_m[f\"W{ℓ}\"] = np.zeros_like(self.params[f\"W{ℓ}\"])\n",
    "            self.adam_v[f\"W{ℓ}\"] = np.zeros_like(self.params[f\"W{ℓ}\"])\n",
    "            self.adam_m[f\"b{ℓ}\"] = np.zeros_like(self.params[f\"b{ℓ}\"])\n",
    "            self.adam_v[f\"b{ℓ}\"] = np.zeros_like(self.params[f\"b{ℓ}\"])\n",
    "        for ℓ in range(1, self.L):\n",
    "            self.adam_m[f\"gamma{ℓ}\"] = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].gamma)\n",
    "            self.adam_v[f\"gamma{ℓ}\"] = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].gamma)\n",
    "            self.adam_m[f\"beta{ℓ}\"]  = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].beta)\n",
    "            self.adam_v[f\"beta{ℓ}\"]  = np.zeros_like(self.batchnorms[f\"BN{ℓ}\"].beta)\n",
    "\n",
    "        self.adam_t = 0\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def relu_grad(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        X: (batch, input_dim)\n",
    "        Returns: A_L (batch,1) and caches intermediate values.\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        A_prev = X\n",
    "        cache[\"A0\"] = X\n",
    "\n",
    "        # Hidden layers ℓ = 1 ... L-1\n",
    "        for ℓ in range(1, self.L):\n",
    "            Wℓ = self.params[f\"W{ℓ}\"]\n",
    "            bℓ = self.params[f\"b{ℓ}\"]\n",
    "\n",
    "            # 1) Linear\n",
    "            Zℓ = A_prev.dot(Wℓ) + bℓ                   # (batch, hidden_dim)\n",
    "\n",
    "            # 2) BatchNorm\n",
    "            BNℓ = self.batchnorms[f\"BN{ℓ}\"]\n",
    "            Zℓ_norm = BNℓ.forward(Zℓ, training=training)  # (batch, hidden_dim)\n",
    "\n",
    "            # 3) ReLU\n",
    "            Aℓ = self.relu(Zℓ_norm)                      # (batch, hidden_dim)\n",
    "\n",
    "            cache[f\"Z{ℓ}\"]      = Zℓ\n",
    "            cache[f\"Znorm{ℓ}\"]  = Zℓ_norm\n",
    "            cache[f\"A{ℓ}\"]      = Aℓ\n",
    "            A_prev = Aℓ\n",
    "\n",
    "        # Output layer ℓ = L\n",
    "        Wℓ = self.params[f\"W{self.L}\"]\n",
    "        bℓ = self.params[f\"b{self.L}\"]\n",
    "        Zℓ = A_prev.dot(Wℓ) + bℓ                        # (batch, 1)\n",
    "        Aℓ = self.sigmoid(Zℓ)                          # (batch, 1)\n",
    "\n",
    "        cache[f\"Z{self.L}\"] = Zℓ\n",
    "        cache[f\"A{self.L}\"] = Aℓ\n",
    "\n",
    "        return Aℓ, cache\n",
    "\n",
    "    def compute_loss_and_grads(self, A_L, Y, cache):\n",
    "        \"\"\"\n",
    "        Compute binary cross‐entropy loss and backprop gradients.\n",
    "        A_L: (batch,1), Y: (batch,1)\n",
    "        cache: intermediate values\n",
    "        Returns: loss, grads dict (dWℓ, dbℓ, dgammaℓ, dbetaℓ)\n",
    "        \"\"\"\n",
    "        m = Y.shape[0]\n",
    "        # Compute loss\n",
    "        A_L_clipped = np.clip(A_L, 1e-8, 1 - 1e-8)\n",
    "        loss = -np.sum(Y * np.log(A_L_clipped) + (1 - Y) * np.log(1 - A_L_clipped)) / m\n",
    "\n",
    "        grads = {}\n",
    "        # -------------------\n",
    "        # Output layer ℓ = L\n",
    "        # -------------------\n",
    "        ZL = cache[f\"Z{self.L}\"]             # (batch,1)\n",
    "        A_prev = cache[f\"A{self.L - 1}\"]     # (batch, hidden_dim of L-1)\n",
    "        dZL = A_L - Y                        # (batch,1)\n",
    "        grads[f\"dW{self.L}\"] = A_prev.T.dot(dZL) / m  # (hidden_dim, 1)\n",
    "        grads[f\"db{self.L}\"] = np.sum(dZL, axis=0, keepdims=True) / m   # (1,1)\n",
    "        dA_prev = dZL.dot(self.params[f\"W{self.L}\"].T)       # (batch, hidden_dim)\n",
    "\n",
    "        # -----------------------------------\n",
    "        # Hidden layers ℓ = L-1 ... 1 (reverse)\n",
    "        # -----------------------------------\n",
    "        for ℓ in reversed(range(1, self.L)):\n",
    "            Zℓ      = cache[f\"Z{ℓ}\"]                # (batch, hidden_dim)\n",
    "            Zℓ_norm = cache[f\"Znorm{ℓ}\"]            # (batch, hidden_dim)\n",
    "            A_prev  = cache[f\"A{ℓ - 1}\"]             # (batch, size of layer ℓ-1)\n",
    "\n",
    "            # 1) Backprop through ReLU\n",
    "            dZ_norm = dA_prev * self.relu_grad(Zℓ_norm)  # (batch, hidden_dim)\n",
    "\n",
    "            # 2) Backprop through BatchNorm\n",
    "            BNℓ = self.batchnorms[f\"BN{ℓ}\"]\n",
    "            dZ  = BNℓ.backward(dZ_norm)                  # (batch, hidden_dim)\n",
    "            grads[f\"dgamma{ℓ}\"] = BNℓ.dgamma             # (1, hidden_dim)\n",
    "            grads[f\"dbeta{ℓ}\"]  = BNℓ.dbeta              # (1, hidden_dim)\n",
    "\n",
    "            # 3) Backprop through linear\n",
    "            grads[f\"dW{ℓ}\"] = A_prev.T.dot(dZ) / m       # (size of layer ℓ-1, hidden_dim)\n",
    "            grads[f\"db{ℓ}\"] = np.sum(dZ, axis=0, keepdims=True) / m     # (1, hidden_dim)\n",
    "            dA_prev = dZ.dot(self.params[f\"W{ℓ}\"].T)     # (batch, size of layer ℓ-1)\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def update_params_adam(self, grads):\n",
    "        \"\"\"\n",
    "        Update all parameters with Adam optimizer.\n",
    "        grads: dictionary containing gradients for dWℓ, dbℓ, dgammaℓ, dbetaℓ.\n",
    "        \"\"\"\n",
    "        self.adam_t += 1\n",
    "        lr_t = self.lr * np.sqrt(1 - self.beta2**self.adam_t) / (1 - self.beta1**self.adam_t)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for ℓ in range(1, self.L + 1):\n",
    "            for param in [f\"W{ℓ}\", f\"b{ℓ}\"]:\n",
    "                grad = grads[f\"d{param}\"]\n",
    "                # Update biased first moment\n",
    "                self.adam_m[param] = self.beta1 * self.adam_m[param] + (1 - self.beta1) * grad\n",
    "                # Update biased second moment\n",
    "                self.adam_v[param] = self.beta2 * self.adam_v[param] + (1 - self.beta2) * (grad ** 2)\n",
    "                # Compute bias‐corrected estimates\n",
    "                m_hat = self.adam_m[param] / (1 - self.beta1**self.adam_t)\n",
    "                v_hat = self.adam_v[param] / (1 - self.beta2**self.adam_t)\n",
    "                # Update parameter\n",
    "                self.params[param] -= lr_t * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "        # Update batchnorm γ and β for ℓ = 1...L-1\n",
    "        for ℓ in range(1, self.L):\n",
    "            for param in [f\"gamma{ℓ}\", f\"beta{ℓ}\"]:\n",
    "                grad_key = \"d\" + param       # e.g. \"dgamma1\" or \"dbeta1\"\n",
    "                grad = grads[grad_key]\n",
    "                self.adam_m[param] = self.beta1 * self.adam_m[param] + (1 - self.beta1) * grad\n",
    "                self.adam_v[param] = self.beta2 * self.adam_v[param] + (1 - self.beta2) * (grad ** 2)\n",
    "                m_hat = self.adam_m[param] / (1 - self.beta1**self.adam_t)\n",
    "                v_hat = self.adam_v[param] / (1 - self.beta2**self.adam_t)\n",
    "                if param.startswith(\"gamma\"):\n",
    "                    ℓ_idx = int(param.replace(\"gamma\", \"\"))\n",
    "                    self.batchnorms[f\"BN{ℓ_idx}\"].gamma -= lr_t * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "                else:  # \"beta\"\n",
    "                    ℓ_idx = int(param.replace(\"beta\", \"\"))\n",
    "                    self.batchnorms[f\"BN{ℓ_idx}\"].beta -= lr_t * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def predict(self, X):\n",
    "        A_L, _ = self.forward(X, training=False)\n",
    "        return (A_L > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4) Training Loop for Deep MLP Binary\n",
    "# ----------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    X, Y = generate_binary_data(n_per_class=500, seed=42)\n",
    "    # Split 80% train, 20% val\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, Y_train = X[:split], Y[:split]\n",
    "    X_val,   Y_val   = X[split:], Y[split:]\n",
    "\n",
    "    # Define network dims: 2→16→16→8→8→1 (4 hidden layers)\n",
    "    layer_dims = [2, 16, 16, 8, 8, 1]\n",
    "    model = DeepMLP(layer_dims, lr=0.001, seed=1)\n",
    "\n",
    "    epochs = 200\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Forward + compute loss + backward\n",
    "        A_L, cache = model.forward(X_train, training=True)\n",
    "        loss, grads = model.compute_loss_and_grads(A_L, Y_train, cache)\n",
    "        model.update_params_adam(grads)\n",
    "\n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            train_preds = model.predict(X_train)\n",
    "            val_preds   = model.predict(X_val)\n",
    "            train_acc = np.mean(train_preds == Y_train)\n",
    "            val_acc   = np.mean(val_preds   == Y_val)\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    train_acc = np.mean(model.predict(X_train) == Y_train)\n",
    "    val_acc   = np.mean(model.predict(X_val)   == Y_val)\n",
    "    print(f\"\\nFinal Train Acc: {train_acc:.4f} | Final Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skhan3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
